{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 丢弃法（Dropout）--- 从0开始\n",
    "\n",
    "前面我们介绍了多层神经网络，就是包含至少一个隐含层的网络。我们也介绍了正则法来应对过拟合问题。在深度学习中，一个常用的应对过拟合问题的方法叫做丢弃法（Dropout）。本节以多层神经网络为例，从0开始介绍丢弃法。\n",
    "\n",
    "由于丢弃法的概念和实现非常容易，在本节中，我们先介绍丢弃法的概念以及它在现代神经网络中是如何实现的。然后我们一起探讨丢弃法的本质。\n",
    "\n",
    "\n",
    "## 丢弃法的概念\n",
    "\n",
    "在现代神经网络中，我们所指的丢弃法，通常是对输入层或者隐含层做以下操作：\n",
    "\n",
    "* 随机选择一部分该层的输出作为丢弃元素；\n",
    "* 把丢弃元素乘以0；\n",
    "* 把非丢弃元素拉伸。\n",
    "\n",
    "\n",
    "## 丢弃法的实现\n",
    "\n",
    "丢弃法的实现很容易，例如像下面这样。这里的标量`drop_probability`定义了一个`X`（`NDArray`类）中任何一个元素被丢弃的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "\n",
    "def dropout(X, drop_probability):\n",
    "    keep_probability = 1 - drop_probability\n",
    "    assert 0 <= keep_probability <= 1\n",
    "    # 这种情况下把全部元素都丢弃。\n",
    "    if keep_probability == 0:\n",
    "        return X.zeros_like()\n",
    "    \n",
    "    # 随机选择一部分该层的输出作为丢弃元素。\n",
    "    mask = nd.random.uniform(\n",
    "        0, 1.0, X.shape, ctx=X.context) < keep_probability\n",
    "    # 保证 E[dropout(X)] == X\n",
    "    scale =  1 / keep_probability \n",
    "    return mask * X * scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们运行几个实例来验证一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[  0.   1.   2.   3.]\n",
       " [  4.   5.   6.   7.]\n",
       " [  8.   9.  10.  11.]\n",
       " [ 12.  13.  14.  15.]\n",
       " [ 16.  17.  18.  19.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = nd.arange(20).reshape((5,4))\n",
    "dropout(A, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[  0.   0.   0.   6.]\n",
       " [  0.  10.   0.   0.]\n",
       " [ 16.  18.  20.   0.]\n",
       " [ 24.  26.   0.   0.]\n",
       " [  0.  34.   0.   0.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(A, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.]\n",
       " [ 0.  0.  0.  0.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(A, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 丢弃法的本质\n",
    "\n",
    "了解了丢弃法的概念与实现，那你可能对它的本质产生了好奇。\n",
    "\n",
    "如果你了解集成学习，你可能知道它在提升弱分类器准确率上的威力。一般来说，在集成学习里，我们可以对训练数据集有放回地采样若干次并分别训练若干个不同的分类器；测试时，把这些分类器的结果集成一下作为最终分类结果。\n",
    "\n",
    "事实上，丢弃法在模拟集成学习。试想，一个使用了丢弃法的多层神经网络本质上是原始网络的子集（节点和边）。举个例子，它可能长这个样子。\n",
    "\n",
    "![](../img/dropout.png)\n",
    "\n",
    "我们在之前的章节里介绍过[随机梯度下降算法](linear-regression-scratch.md)：我们在训练神经网络模型时一般随机采样一个批量的训练数\n",
    "据。丢弃法实质上是对每一个这样的数据集分别训练一个原神经网络子集的分类器。与一般的集成学习不同，这里每个原神经网络子集的分类器用的是同一套参数。因此丢弃法只是在模拟集成学习。\n",
    "\n",
    "我们刚刚强调了，原神经网络子集的分类器在不同的训练数据批量上训练并使用同一套参数。因此，使用丢弃法的神经网络实质上是对输入层和隐含层的参数做了正则化：学到的参数使得原神经网络不同子集在训练数据上都尽可能表现良好。\n",
    "\n",
    "下面我们动手实现一下在多层神经网络里加丢弃层。\n",
    "\n",
    "## 数据获取\n",
    "\n",
    "我们继续使用FashionMNIST数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "class DataLoader(object):\n",
    "    \"\"\"similiar to gluon.data.DataLoader, but might be faster.\n",
    "\n",
    "    The main difference this data loader tries to read more exmaples each\n",
    "    time. But the limits are 1) all examples in dataset have the same shape, 2)\n",
    "    data transfomer needs to process multiple examples at each time\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, shuffle, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.transform = transform\n",
    "\n",
    "    def __iter__(self):\n",
    "        data = self.dataset[:]\n",
    "        X = data[0]\n",
    "        y = nd.array(data[1])\n",
    "        n = X.shape[0]\n",
    "        if self.shuffle:\n",
    "            idx = np.arange(n)\n",
    "            np.random.shuffle(idx)\n",
    "            X = nd.array(X.asnumpy()[idx])\n",
    "            y = nd.array(y.asnumpy()[idx])\n",
    "\n",
    "        for i in range(n//self.batch_size):\n",
    "            if self.transform is not None:\n",
    "                yield self.transform(X[i*self.batch_size:(i+1)*self.batch_size], \n",
    "                                     y[i*self.batch_size:(i+1)*self.batch_size])\n",
    "            else:\n",
    "                yield (X[i*self.batch_size:(i+1)*self.batch_size],\n",
    "                       y[i*self.batch_size:(i+1)*self.batch_size])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)//self.batch_size\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=\"~/.mxnet/datasets/fashion-mnist\"):\n",
    "    \"\"\"download the fashion mnist dataest and then load into memory\"\"\"\n",
    "    def transform_mnist(data, label):\n",
    "        # Transform a batch of examples.\n",
    "        if resize:\n",
    "            n = data.shape[0]\n",
    "            new_data = nd.zeros((n, resize, resize, data.shape[3]))\n",
    "            for i in range(n):\n",
    "                new_data[i] = image.imresize(data[i], resize, resize)\n",
    "            data = new_data\n",
    "        # change data from batch x height x width x channel to batch x channel x height x width\n",
    "        return nd.transpose(data.astype('float32'), (0,3,1,2))/255, label.astype('float32')\n",
    "\n",
    "    mnist_train = gluon.data.vision.FashionMNIST(root=root, train=True, transform=None)\n",
    "    mnist_test = gluon.data.vision.FashionMNIST(root=root, train=False, transform=None)\n",
    "    # Transform later to avoid memory explosion. \n",
    "    train_data = DataLoader(mnist_train, batch_size, shuffle=True, transform=transform_mnist)\n",
    "    test_data = DataLoader(mnist_test, batch_size, shuffle=False, transform=transform_mnist)\n",
    "    return (train_data, test_data)\n",
    "\n",
    "batch_size = 256\n",
    "train_data, test_data = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 含两个隐藏层的多层感知机\n",
    "\n",
    "[多层感知机](mlp-scratch.md)已经在之前章节里介绍。与[之前章节](mlp-scratch.md)不同，这里我们定义一个包含两个隐含层的模型，两个隐含层都输出256个节点。我们定义激活函数Relu并直接使用Gluon提供的交叉熵损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "\n",
    "num_inputs = 28*28\n",
    "num_outputs = 10\n",
    "\n",
    "num_hidden1 = 256\n",
    "num_hidden2 = 256\n",
    "weight_scale = .01\n",
    "\n",
    "W1 = nd.random_normal(shape=(num_inputs, num_hidden1), scale=weight_scale)\n",
    "b1 = nd.zeros(num_hidden1)\n",
    "\n",
    "W2 = nd.random_normal(shape=(num_hidden1, num_hidden2), scale=weight_scale)\n",
    "b2 = nd.zeros(num_hidden2)\n",
    "\n",
    "W3 = nd.random_normal(shape=(num_hidden2, num_outputs), scale=weight_scale)\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义包含丢弃层的模型\n",
    "\n",
    "我们的模型就是将层（全连接）和激活函数（Relu）串起来，并在应用激活函数后添加丢弃层。每个丢弃层的元素丢弃概率可以分别设置。一般情况下，我们推荐把更靠近输入层的元素丢弃概率设的更小一点。这个试验中，我们把第一层全连接后的元素丢弃概率设为0.2，把第二层全连接后的元素丢弃概率设为0.5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_prob1 = 0.5\n",
    "drop_prob2 = 0.2\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    # 第一层全连接。\n",
    "    h1 = nd.relu(nd.dot(X, W1) + b1)\n",
    "    # 在第一层全连接后添加丢弃层。\n",
    "    h1 = dropout(h1, drop_prob1)\n",
    "    # 第二层全连接。\n",
    "    h2 = nd.relu(nd.dot(h1, W2) + b2)\n",
    "    # 在第二层全连接后添加丢弃层。\n",
    "    h2 = dropout(h2, drop_prob2)\n",
    "    return nd.dot(h2, W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] -= lr * param.grad\n",
    "        \n",
    "def accuracy(y_hy, y_true):\n",
    "    return nd.mean(y_hy.argmax(axis=1)==y_true).asscalar()\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = 0\n",
    "    for data, label in data_iterator:\n",
    "        output = net(data)\n",
    "        acc += accuracy(output, label)\n",
    "    return acc / len(data_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "训练跟之前一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.376796, Train acc 0.862230, Test acc 0.848257\n",
      "Epoch 1. Loss: 0.372187, Train acc 0.863415, Test acc 0.843850\n",
      "Epoch 2. Loss: 0.364014, Train acc 0.865067, Test acc 0.863682\n",
      "Epoch 3. Loss: 0.361312, Train acc 0.867505, Test acc 0.853566\n",
      "Epoch 4. Loss: 0.355237, Train acc 0.869341, Test acc 0.865585\n"
     ]
    }
   ],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "learning_rate = .5\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate/batch_size)\n",
    "\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += accuracy(output, label)\n",
    "\n",
    "    test_acc = evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "        epoch, train_loss/len(train_data), \n",
    "        train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout (0.2, 0.5):\n",
    "Epoch 0. Loss: 0.591819, Train acc 0.778579, Test acc 0.815905\n",
    "Epoch 1. Loss: 0.500449, Train acc 0.815438, Test acc 0.841947\n",
    "Epoch 2. Loss: 0.453552, Train acc 0.832799, Test acc 0.834736\n",
    "Epoch 3. Loss: 0.422421, Train acc 0.845603, Test acc 0.840244\n",
    "Epoch 4. Loss: 0.403465, Train acc 0.853616, Test acc 0.851162\n",
    "\n",
    "dropout (0.1, 0.5):\n",
    "Epoch 0. Loss: 0.332491, Train acc 0.878923, Test acc 0.866987\n",
    "Epoch 1. Loss: 0.312915, Train acc 0.884582, Test acc 0.872196\n",
    "Epoch 2. Loss: 0.307689, Train acc 0.887386, Test acc 0.869191\n",
    "Epoch 3. Loss: 0.299824, Train acc 0.890775, Test acc 0.879107\n",
    "Epoch 4. Loss: 0.295716, Train acc 0.891176, Test acc 0.873197\n",
    "\n",
    "not dropout:\n",
    "Epoch 0. Loss: 0.345876, Train acc 0.871110, Test acc 0.866486\n",
    "Epoch 1. Loss: 0.327870, Train acc 0.877170, Test acc 0.877304\n",
    "Epoch 2. Loss: 0.316321, Train acc 0.881093, Test acc 0.870092\n",
    "Epoch 3. Loss: 0.303353, Train acc 0.885734, Test acc 0.879006\n",
    "Epoch 4. Loss: 0.296504, Train acc 0.888288, Test acc 0.867688\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "我们可以通过使用丢弃法对神经网络正则化。\n",
    "\n",
    "## 练习\n",
    "\n",
    "- 尝试不使用丢弃法，看看这个包含两个隐含层的多层感知机可以得到什么结果。\n",
    "- 我们推荐把更靠近输入层的元素丢弃概率设的更小一点。想想这是为什么？如果把本节教程中的两个元素丢弃参数对调会有什么结果？\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/1278)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
