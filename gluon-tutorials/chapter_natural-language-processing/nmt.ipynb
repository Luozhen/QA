{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经机器翻译\n",
    "\n",
    "本节介绍[编码器—解码器和注意力机制](seq2seq-attention.md)的应用。我们以神经机器翻译（neural machine translation）为例，介绍如何使用Gluon实现一个简单的编码器—解码器和注意力机制模型。\n",
    "\n",
    "\n",
    "## 使用Gluon实现编码器—解码器和注意力机制\n",
    "\n",
    "我们先载入需要的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, nd\n",
    "from mxnet.gluon import nn, rnn, Block\n",
    "from mxnet.contrib import text\n",
    "\n",
    "from io import open\n",
    "import collections\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义一些特殊字符。其中PAD (padding)符号使每个序列等长；BOS (beginning of sequence)符号表示序列的开始；而EOS (end of sequence)符号表示序列的结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "BOS = '<bos>'\n",
    "EOS = '<eos>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是一些可以调节的模型参数。我们在编码器和解码器中分别使用了一层和两层的循环神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "epoch_period = 10\n",
    "\n",
    "learning_rate = 0.005\n",
    "# 输入或输出序列的最大长度（含句末添加的EOS字符）。\n",
    "max_seq_len = 5\n",
    "\n",
    "encoder_num_layers = 1\n",
    "decoder_num_layers = 2\n",
    "\n",
    "encoder_drop_prob = 0.1\n",
    "decoder_drop_prob = 0.1\n",
    "\n",
    "encoder_hidden_dim = 256\n",
    "decoder_hidden_dim = 256\n",
    "alignment_dim = 25\n",
    "\n",
    "ctx = mx.cpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据\n",
    "\n",
    "我们定义函数读取训练数据集。为了减少运行时间，我们使用一个很小的法语——英语数据集。\n",
    "\n",
    "这里使用了[之前章节](pretrained-embedding.md)介绍的`mxnet.contrib.text`来创建法语和英语的词典。需要注意的是，我们会在句末附上EOS符号，并可能通过添加PAD符号使每个序列等长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(max_seq_len):\n",
    "    input_tokens = []\n",
    "    output_tokens = []\n",
    "    input_seqs = []\n",
    "    output_seqs = []\n",
    "\n",
    "    with open('../data/fr-en-small.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            input_seq, output_seq = line.rstrip().split('\\t')\n",
    "            cur_input_tokens = input_seq.split(' ')\n",
    "            cur_output_tokens = output_seq.split(' ')\n",
    "\n",
    "            if len(cur_input_tokens) < max_seq_len and \\\n",
    "                            len(cur_output_tokens) < max_seq_len:\n",
    "                input_tokens.extend(cur_input_tokens)\n",
    "                # 句末附上EOS符号。\n",
    "                cur_input_tokens.append(EOS)\n",
    "                # 添加PAD符号使每个序列等长（长度为max_seq_len）。\n",
    "                while len(cur_input_tokens) < max_seq_len:\n",
    "                    cur_input_tokens.append(PAD)\n",
    "                input_seqs.append(cur_input_tokens)\n",
    "                output_tokens.extend(cur_output_tokens)\n",
    "                cur_output_tokens.append(EOS)\n",
    "                while len(cur_output_tokens) < max_seq_len:\n",
    "                    cur_output_tokens.append(PAD)\n",
    "                output_seqs.append(cur_output_tokens)\n",
    "\n",
    "        fr_vocab = text.vocab.Vocabulary(collections.Counter(input_tokens),\n",
    "                                         reserved_tokens=[PAD, BOS, EOS])\n",
    "        en_vocab = text.vocab.Vocabulary(collections.Counter(output_tokens),\n",
    "                                         reserved_tokens=[PAD, BOS, EOS])\n",
    "    return fr_vocab, en_vocab, input_seqs, output_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下创建训练数据集。每一个样本包含法语的输入序列和英语的输出序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'elle', u'est', u'vieille', u'.', '<eos>'], [u'elle', u'est', u'tranquille', u'.', '<eos>'], [u'elle', u'a', u'tort', u'.', '<eos>'], [u'elle', u'est', u'canadienne', u'.', '<eos>'], [u'elle', u'est', u'japonaise', u'.', '<eos>'], [u'ils', u'sont', u'russes', u'.', '<eos>'], [u'ils', u'se', u'disputent', u'.', '<eos>'], [u'ils', u'regardent', u'.', '<eos>', '<pad>'], [u'ils', u'sont', u'acteurs', u'.', '<eos>'], [u'elles', u'sont', u'crevees', u'.', '<eos>']]\n",
      "\n",
      "[[  5.   6.  21.   4.   3.]\n",
      " [  5.   6.  20.   4.   3.]\n",
      " [  5.   9.  19.   4.   3.]\n",
      " [  5.   6.  11.   4.   3.]\n",
      " [  5.   6.  15.   4.   3.]\n",
      " [  7.   8.  17.   4.   3.]\n",
      " [  7.  18.  13.   4.   3.]\n",
      " [  7.  16.   4.   3.   1.]\n",
      " [  7.   8.  10.   4.   3.]\n",
      " [ 14.   8.  12.   4.   3.]]\n",
      "<NDArray 10x5 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "input_vocab, output_vocab, input_seqs, output_seqs = read_data(max_seq_len)\n",
    "X = nd.zeros((len(input_seqs), max_seq_len), ctx=ctx)\n",
    "Y = nd.zeros((len(output_seqs), max_seq_len), ctx=ctx)\n",
    "for i in range(len(input_seqs)):\n",
    "    X[i] = nd.array(input_vocab.to_indices(input_seqs[i]), ctx=ctx)\n",
    "    Y[i] = nd.array(output_vocab.to_indices(output_seqs[i]), ctx=ctx)\n",
    "\n",
    "dataset = gluon.data.ArrayDataset(X, Y)\n",
    "\n",
    "\n",
    "print input_seqs\n",
    "print X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器、含注意力机制的解码器和解码器初始状态\n",
    "\n",
    "以下定义了基于[GRU](../chapter_recurrent-neural-networks/gru-scratch.md)的编码器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(Block):\n",
    "    \"\"\"编码器\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, drop_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            self.rnn = rnn.GRU(hidden_dim, num_layers, dropout=drop_prob,\n",
    "                               input_size=hidden_dim)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # inputs尺寸: (1, num_steps)，emb尺寸: (num_steps, 1, 256)\n",
    "        emb = self.embedding(inputs).swapaxes(0, 1)\n",
    "        emb = self.dropout(emb)\n",
    "        output, state = self.rnn(emb, state)\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下定义了基于[GRU](../chapter_recurrent-neural-networks/gru-scratch.md)的解码器。它包含[上一节里介绍的注意力机制](seq2seq-attention.md)的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(Block):\n",
    "    \"\"\"含注意力机制的解码器\"\"\"\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers, max_seq_len,\n",
    "                 drop_prob, alignment_dim, encoder_hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            # 注意力机制。\n",
    "            self.attention = nn.Sequential()\n",
    "            with self.attention.name_scope():\n",
    "                self.attention.add(nn.Dense(\n",
    "                    alignment_dim, in_units=hidden_dim + encoder_hidden_dim,\n",
    "                    activation=\"tanh\", flatten=False))\n",
    "                self.attention.add(nn.Dense(1, in_units=alignment_dim,\n",
    "                                            flatten=False))\n",
    "\n",
    "            self.rnn = rnn.GRU(hidden_dim, num_layers, dropout=drop_prob,\n",
    "                               input_size=hidden_dim)\n",
    "            self.out = nn.Dense(output_dim, in_units=hidden_dim)\n",
    "            self.rnn_concat_input = nn.Dense(\n",
    "                hidden_dim, in_units=hidden_dim + encoder_hidden_dim,\n",
    "                flatten=False)\n",
    "\n",
    "    def forward(self, cur_input, state, encoder_outputs):\n",
    "        # 当RNN为多层时，取最靠近输出层的单层隐含状态。\n",
    "        single_layer_state = [state[0][-1].expand_dims(0)]\n",
    "        encoder_outputs = encoder_outputs.reshape((self.max_seq_len, 1,\n",
    "                                                   self.encoder_hidden_dim))\n",
    "        # single_layer_state尺寸: [(1, 1, decoder_hidden_dim)]\n",
    "        # hidden_broadcast尺寸: (max_seq_len, 1, decoder_hidden_dim)\n",
    "        hidden_broadcast = nd.broadcast_axis(single_layer_state[0], axis=0,\n",
    "                                             size=self.max_seq_len)\n",
    "\n",
    "        # encoder_outputs_and_hiddens尺寸:\n",
    "        # (max_seq_len, 1, encoder_hidden_dim + decoder_hidden_dim)\n",
    "        encoder_outputs_and_hiddens = nd.concat(encoder_outputs,\n",
    "                                                hidden_broadcast, dim=2)\n",
    "\n",
    "        # energy尺寸: (max_seq_len, 1, 1)\n",
    "        energy = self.attention(encoder_outputs_and_hiddens)\n",
    "\n",
    "        batch_attention = nd.softmax(energy, axis=0).reshape(\n",
    "            (1, 1, self.max_seq_len))\n",
    "\n",
    "        # batch_encoder_outputs尺寸: (1, max_seq_len, encoder_hidden_dim)\n",
    "        batch_encoder_outputs = encoder_outputs.swapaxes(0, 1)\n",
    "\n",
    "        # decoder_context尺寸: (1, 1, encoder_hidden_dim)\n",
    "        decoder_context = nd.batch_dot(batch_attention, batch_encoder_outputs)\n",
    "\n",
    "        # input_and_context尺寸: (1, 1, encoder_hidden_dim + decoder_hidden_dim)\n",
    "        input_and_context = nd.concat(self.embedding(cur_input).reshape(\n",
    "            (1, 1, self.hidden_size)), decoder_context, dim=2)\n",
    "        # concat_input尺寸: (1, 1, decoder_hidden_dim)\n",
    "        concat_input = self.rnn_concat_input(input_and_context)\n",
    "        concat_input = self.dropout(concat_input)\n",
    "\n",
    "        # 当RNN为多层时，用单层隐含状态初始化各个层的隐含状态。\n",
    "        state = [nd.broadcast_axis(single_layer_state[0], axis=0,\n",
    "                                   size=self.num_layers)]\n",
    "\n",
    "        output, state = self.rnn(concat_input, state)\n",
    "        output = self.dropout(output)\n",
    "        output = self.out(output)\n",
    "        # output尺寸: (1, output_size)，hidden尺寸: [(1, 1, decoder_hidden_dim)]\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了初始化解码器的隐含状态，我们通过一层全连接网络来转化编码器的输出隐含状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderInitState(Block):\n",
    "    \"\"\"解码器隐含状态的初始化\"\"\"\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super(DecoderInitState, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.dense = nn.Dense(decoder_hidden_dim,\n",
    "                                  in_units=encoder_hidden_dim,\n",
    "                                  activation=\"tanh\", flatten=False)\n",
    "\n",
    "    def forward(self, encoder_state):\n",
    "        return [self.dense(encoder_state)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练和应用模型\n",
    "\n",
    "我们定义`translate`函数来应用训练好的模型。这些模型通过该函数的前三个参数传递。解码器的最初时刻输入来自BOS字符。当任一时刻的输出为EOS字符时，输出序列即完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, decoder_init_state, fr_ens, ctx, max_seq_len):\n",
    "    for fr_en in fr_ens:\n",
    "        print('Input :', fr_en[0])\n",
    "        input_tokens = fr_en[0].split(' ') + [EOS]\n",
    "        # 添加PAD符号使每个序列等长（长度为max_seq_len）。\n",
    "        while len(input_tokens) < max_seq_len:\n",
    "            input_tokens.append(PAD)\n",
    "        inputs = nd.array(input_vocab.to_indices(input_tokens), ctx=ctx)\n",
    "        encoder_state = encoder.begin_state(func=mx.nd.zeros, batch_size=1,\n",
    "                                            ctx=ctx)\n",
    "        encoder_outputs, encoder_state = encoder(inputs.expand_dims(0),\n",
    "                                                 encoder_state)\n",
    "        encoder_outputs = encoder_outputs.flatten()\n",
    "        # 解码器的第一个输入为BOS字符。\n",
    "        decoder_input = nd.array([output_vocab.token_to_idx[BOS]], ctx=ctx)\n",
    "        decoder_state = decoder_init_state(encoder_state[0])\n",
    "        output_tokens = []\n",
    "\n",
    "        for i in range(max_seq_len):\n",
    "            decoder_output, decoder_state = decoder(\n",
    "                decoder_input, decoder_state, encoder_outputs)\n",
    "            pred_i = int(decoder_output.argmax(axis=1).asnumpy())\n",
    "            # 当任一时刻的输出为EOS字符时，输出序列即完成。\n",
    "            if pred_i == output_vocab.token_to_idx[EOS]:\n",
    "                break\n",
    "            else:\n",
    "                output_tokens.append(output_vocab.idx_to_token[pred_i])\n",
    "            decoder_input = nd.array([pred_i], ctx=ctx)\n",
    "\n",
    "        print('Output:', ' '.join(output_tokens))\n",
    "        print('Expect:', fr_en[1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义模型训练函数。为了初始化解码器的隐含状态，我们通过一层全连接网络来转化编码器最早时刻的输出隐含状态。这里的解码器使用当前时刻的预测结果作为下一时刻的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, decoder_init_state, max_seq_len, ctx, eval_fr_ens):\n",
    "    # 对于三个网络，分别初始化它们的模型参数并定义它们的优化器。\n",
    "    encoder.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    decoder.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    decoder_init_state.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    encoder_optimizer = gluon.Trainer(encoder.collect_params(), 'adam',\n",
    "                                      {'learning_rate': learning_rate})\n",
    "    decoder_optimizer = gluon.Trainer(decoder.collect_params(), 'adam',\n",
    "                                      {'learning_rate': learning_rate})\n",
    "    decoder_init_state_optimizer = gluon.Trainer(\n",
    "        decoder_init_state.collect_params(), 'adam',\n",
    "        {'learning_rate': learning_rate})\n",
    "\n",
    "    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    prev_time = datetime.datetime.now()\n",
    "    data_iter = gluon.data.DataLoader(dataset, 1, shuffle=True)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for x, y in data_iter:\n",
    "            with autograd.record():\n",
    "                loss = nd.array([0], ctx=ctx)\n",
    "                encoder_state = encoder.begin_state(\n",
    "                    func=mx.nd.zeros, batch_size=1, ctx=ctx)\n",
    "                encoder_outputs, encoder_state = encoder(x, encoder_state)\n",
    "\n",
    "                # encoder_outputs尺寸: (max_seq_len, encoder_hidden_dim)\n",
    "                encoder_outputs = encoder_outputs.flatten()\n",
    "                # 解码器的第一个输入为BOS字符。\n",
    "                decoder_input = nd.array([output_vocab.token_to_idx[BOS]],\n",
    "                                         ctx=ctx)\n",
    "                decoder_state = decoder_init_state(encoder_state[0])\n",
    "                for i in range(max_seq_len):\n",
    "                    decoder_output, decoder_state = decoder(\n",
    "                        decoder_input, decoder_state, encoder_outputs)\n",
    "                    # 解码器使用当前时刻的预测结果作为下一时刻的输入。\n",
    "                    decoder_input = nd.array(\n",
    "                        [decoder_output.argmax(axis=1).asscalar()], ctx=ctx)\n",
    "                    loss = loss + softmax_cross_entropy(decoder_output, y[0][i])\n",
    "                    if y[0][i].asscalar() == output_vocab.token_to_idx[EOS]:\n",
    "                        break\n",
    "\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step(1)\n",
    "            decoder_optimizer.step(1)\n",
    "            decoder_init_state_optimizer.step(1)\n",
    "            total_loss += loss.asscalar() / max_seq_len\n",
    "\n",
    "        if epoch % epoch_period == 0 or epoch == 1:\n",
    "            cur_time = datetime.datetime.now()\n",
    "            h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "            m, s = divmod(remainder, 60)\n",
    "            time_str = 'Time %02d:%02d:%02d' % (h, m, s)\n",
    "            if epoch == 1:\n",
    "                print_loss_avg = total_loss / len(data_iter)\n",
    "            else:\n",
    "                print_loss_avg = total_loss / epoch_period / len(data_iter)\n",
    "            loss_str = 'Epoch %d, Loss %f, ' % (epoch, print_loss_avg)\n",
    "            print(loss_str + time_str)\n",
    "            if epoch != 1:\n",
    "                total_loss = 0.0\n",
    "            prev_time = cur_time\n",
    "\n",
    "            translate(encoder, decoder, decoder_init_state, eval_fr_ens, ctx,\n",
    "                      max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下分别实例化编码器、解码器和解码器初始隐含状态网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(input_vocab), encoder_hidden_dim, encoder_num_layers,\n",
    "                  encoder_drop_prob)\n",
    "decoder = Decoder(decoder_hidden_dim, len(output_vocab),\n",
    "                  decoder_num_layers, max_seq_len, decoder_drop_prob,\n",
    "                  alignment_dim, encoder_hidden_dim)\n",
    "decoder_init_state = DecoderInitState(encoder_hidden_dim, decoder_hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定简单的法语和英语序列，我们可以观察模型的训练结果。打印的结果中，Input、Output和Expect分别代表输入序列、输出序列和正确序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 2.881975, Time 00:00:00\n",
      "('Input :', 'elle est japonaise .')\n",
      "('Output:', u'she japanese .')\n",
      "('Expect:', 'she is japanese .', '\\n')\n",
      "('Input :', 'ils regardent .')\n",
      "('Output:', u'they . .')\n",
      "('Expect:', 'they are watching .', '\\n')\n",
      "Epoch 10, Loss 1.047021, Time 00:00:04\n",
      "('Input :', 'elle est japonaise .')\n",
      "('Output:', u'she is japanese .')\n",
      "('Expect:', 'she is japanese .', '\\n')\n",
      "('Input :', 'ils regardent .')\n",
      "('Output:', u'they are watching .')\n",
      "('Expect:', 'they are watching .', '\\n')\n",
      "Epoch 20, Loss 0.410030, Time 00:00:05\n",
      "('Input :', 'elle est japonaise .')\n",
      "('Output:', u'she is japanese .')\n",
      "('Expect:', 'she is japanese .', '\\n')\n",
      "('Input :', 'ils regardent .')\n",
      "('Output:', u'they are watching .')\n",
      "('Expect:', 'they are watching .', '\\n')\n",
      "Epoch 30, Loss 0.399794, Time 00:00:05\n",
      "('Input :', 'elle est japonaise .')\n",
      "('Output:', u'she is japanese .')\n",
      "('Expect:', 'she is japanese .', '\\n')\n",
      "('Input :', 'ils regardent .')\n",
      "('Output:', u'they are watching .')\n",
      "('Expect:', 'they are watching .', '\\n')\n",
      "Epoch 40, Loss 0.036789, Time 00:00:05\n",
      "('Input :', 'elle est japonaise .')\n",
      "('Output:', u'she is japanese .')\n",
      "('Expect:', 'she is japanese .', '\\n')\n",
      "('Input :', 'ils regardent .')\n",
      "('Output:', u'they are watching .')\n",
      "('Expect:', 'they are watching .', '\\n')\n",
      "Epoch 50, Loss 0.001709, Time 00:00:05\n",
      "('Input :', 'elle est japonaise .')\n",
      "('Output:', u'she is japanese .')\n",
      "('Expect:', 'she is japanese .', '\\n')\n",
      "('Input :', 'ils regardent .')\n",
      "('Output:', u'they are watching .')\n",
      "('Expect:', 'they are watching .', '\\n')\n"
     ]
    }
   ],
   "source": [
    "eval_fr_ens =[['elle est japonaise .', 'she is japanese .'],\n",
    "              ['ils regardent .', 'they are watching .']]\n",
    "train(encoder, decoder, decoder_init_state, max_seq_len, ctx, eval_fr_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 束搜索\n",
    "\n",
    "在上一节里，我们提到编码器最终输出了一个背景向量$\\mathbf{c}$，该背景向量编码了输入序列$x_1, x_2, \\ldots, x_T$的信息。假设训练数据中的输出序列是$y_1, y_2, \\ldots, y_{T^\\prime}$，输出序列的生成概率是\n",
    "\n",
    "$$\\mathbb{P}(y_1, \\ldots, y_{T^\\prime}) = \\prod_{t^\\prime=1}^{T^\\prime} \\mathbb{P}(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\mathbf{c})$$\n",
    "\n",
    "\n",
    "对于机器翻译的输出来说，如果输出语言的词汇集合$\\mathcal{Y}$的大小为$|\\mathcal{Y}|$，输出序列的长度为$T^\\prime$，那么可能的输出序列种类是$\\mathcal{O}(|\\mathcal{Y}|^{T^\\prime})$。为了找到生成概率最大的输出序列，一种方法是计算所有$\\mathcal{O}(|\\mathcal{Y}|^{T^\\prime})$种可能序列的生成概率，并输出概率最大的序列。我们将该序列称为最优序列。但是这种方法的计算开销过高（例如，$10000^{10} = 1 \\times 10^{40}$）。\n",
    "\n",
    "\n",
    "我们目前所介绍的解码器在每个时刻只输出生成概率最大的一个词汇。对于任一时刻$t^\\prime$，我们从$|\\mathcal{Y}|$个词中搜索出输出词\n",
    "\n",
    "$$y_{t^\\prime} = \\text{argmax}_{y_{t^\\prime} \\in \\mathcal{Y}} \\mathbb{P}(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\mathbf{c})$$\n",
    "\n",
    "因此，搜索计算开销（$\\mathcal{O}(|\\mathcal{Y}| \\times {T^\\prime})$）显著下降（例如，$10000 \\times 10 = 1 \\times 10^5$），但这并不能保证一定搜索到最优序列。\n",
    "\n",
    "束搜索（beam search）介于上面二者之间。我们来看一个例子。\n",
    "\n",
    "假设输出序列的词典中只包含五个词：$\\mathcal{Y} = \\{A, B, C, D, E\\}$。束搜索的一个超参数叫做束宽（beam width）。以束宽等于2为例，假设输出序列长度为3，假如时刻1生成概率$\\mathbb{P}(y_{t^\\prime} \\mid \\mathbf{c})$最大的两个词为$A$和$C$，我们在时刻2对于所有的$y_2 \\in \\mathcal{Y}$都分别计算$\\mathbb{P}(y_2 \\mid A, \\mathbf{c})$和$\\mathbb{P}(y_2 \\mid C, \\mathbf{c})$，从计算出的10个概率中取最大的两个，假设为$\\mathbb{P}(B \\mid A, \\mathbf{c})$和$\\mathbb{P}(E \\mid C, \\mathbf{c})$。那么，我们在时刻3对于所有的$y_3 \\in \\mathcal{Y}$都分别计算$\\mathbb{P}(y_3 \\mid A, B, \\mathbf{c})$和$\\mathbb{P}(y_3 \\mid C, E, \\mathbf{c})$，从计算出的10个概率中取最大的两个，假设为$\\mathbb{P}(D \\mid A, B, \\mathbf{c})$和$\\mathbb{P}(D \\mid C, E, \\mathbf{c})$。\n",
    "\n",
    "接下来，我们可以在输出序列：$A$、$C$、$AB$、$CE$、$ABD$、$CED$中筛选出以特殊字符EOS结尾的候选序列。再在候选序列中取以下分数最高的序列作为最终候选序列：\n",
    "\n",
    "$$ \\frac{1}{L^\\alpha} \\log \\mathbb{P}(y_1, \\ldots, y_{L}) = \\frac{1}{L^\\alpha} \\sum_{t^\\prime=1}^L \\log \\mathbb{P}(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\mathbf{c})$$\n",
    "\n",
    "其中$L$为候选序列长度，$\\alpha$一般可选为0.75。分母上的$L^\\alpha$是为了惩罚较长序列的分数中的对数相加项。\n",
    "\n",
    "## 评价翻译结果\n",
    "\n",
    "2002年，IBM团队提出了一种评价翻译结果的指标，叫做[BLEU](https://www.aclweb.org/anthology/P02-1040.pdf) （Bilingual Evaluation Understudy）。\n",
    "\n",
    "设$k$为我们希望评价的n-gram的最大长度，例如$k=4$。n-gram的精度$p_n$为模型输出中的n-gram匹配参考输出的数量与模型输出中的n-gram的数量的比值。例如，参考输出（真实值）为ABCDEF，模型输出为ABBCD。那么$p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0$。设$len_{ref}$和$len_{MT}$分别为参考输出和模型输出的词数。那么，BLEU的定义为\n",
    "\n",
    "$$ \\exp(\\min(0, 1 - \\frac{len_{ref}}{len_{MT}})) \\prod_{i=1}^k p_n^{1/2^n}$$\n",
    "\n",
    "需要注意的是，随着$n$的提高，n-gram的精度的权值随着$p_n^{1/2^n}$中的指数减小而提高。例如$0.5^{1/2} \\approx 0.7, 0.5^{1/4} \\approx 0.84, 0.5^{1/8} \\approx 0.92, 0.5^{1/16} \\approx 0.96$。换句话说，匹配4-gram比匹配1-gram应该得到更多奖励。另外，模型输出越短往往越容易得到较高的n-gram的精度。因此，BLEU公式里连乘项前面的系数为了惩罚较短的输出。例如当$k=2$时，参考输出为ABCDEF，而模型输出为AB，此时的$p_1 = p_2 = 1$，而$\\exp(1-6/3) \\approx 0.37$，因此BLEU=0.37。当模型输出也为ABCDEF时，BLEU=1。\n",
    "\n",
    "## 结论\n",
    "\n",
    "* 我们可以将编码器—解码器和注意力机制应用于神经机器翻译中。\n",
    "* 束搜索有可能提高输出质量。\n",
    "* BLEU可以用来评价翻译结果。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 试着使用更大的翻译数据集来训练模型，例如[WMT](http://www.statmt.org/wmt14/translation-task.html)和[Tatoeba Project](http://www.manythings.org/anki/)。调一调不同参数并观察实验结果。\n",
    "* Teacher forcing：在模型训练中，试着让解码器使用当前时刻的正确结果（而不是预测结果）作为下一时刻的输入。结果会怎么样？\n",
    "\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/4689)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}