{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更深的卷积神经网络：GoogLeNet\n",
    "\n",
    "在2014年的Imagenet竞赛里，Google的研究人员利用一个新的网络结构取得很大的优先。这个叫做GoogLeNet的网络虽然在名字上是向LeNet致敬，但网络结构里很难看到LeNet的影子。它颠覆的大家对卷积神经网络串联一系列层的固定做法。下图是其[论文](https://arxiv.org/abs/1409.4842)对GoogLeNet的可视化\n",
    "\n",
    "![](../img/googlenet.png)\n",
    "\n",
    "## 定义Inception\n",
    "\n",
    "可以看到其中有多个四个并行卷积层的块。这个块一般叫做Inception，其基于[Network in network](./nin-gluon.md)的思想做了很大的改进。我们先看下如何定义一个下图所示的Inception块。\n",
    "\n",
    "![](../img/inception.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "\n",
    "class Inception(nn.Block):\n",
    "    def __init__(self, n1_1, n2_1, n2_3, n3_1, n3_5, n4_1, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # path 1\n",
    "        self.p1_conv_1 = nn.Conv2D(n1_1, kernel_size=1,\n",
    "                                   activation='relu')\n",
    "        # path 2\n",
    "        self.p2_conv_1 = nn.Conv2D(n2_1, kernel_size=1,\n",
    "                                   activation='relu')\n",
    "        self.p2_conv_3 = nn.Conv2D(n2_3, kernel_size=3, padding=1,\n",
    "                                   activation='relu')\n",
    "        # path 3\n",
    "        self.p3_conv_1 = nn.Conv2D(n3_1, kernel_size=1,\n",
    "                                   activation='relu')\n",
    "        self.p3_conv_5 = nn.Conv2D(n3_5, kernel_size=5, padding=2,\n",
    "                                   activation='relu')\n",
    "        # path 4\n",
    "        self.p4_pool_3 = nn.MaxPool2D(pool_size=3, padding=1,\n",
    "                                      strides=1)\n",
    "        self.p4_conv_1 = nn.Conv2D(n4_1, kernel_size=1,\n",
    "                                   activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_conv_1(x)\n",
    "        p2 = self.p2_conv_3(self.p2_conv_1(x))\n",
    "        p3 = self.p3_conv_5(self.p3_conv_1(x))\n",
    "        p4 = self.p4_conv_1(self.p4_pool_3(x))\n",
    "        return nd.concat(p1, p2, p3, p4, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到Inception里有四个并行的线路。\n",
    "\n",
    "1. 单个$1\\times 1$卷积。\n",
    "2. $1\\times 1$卷积接上$3\\times 3$卷积。通常前者的通道数少于输入通道，这样减少后者的计算量。后者加上了`padding=1`使得输出的长宽的输入一致\n",
    "3. 同2，但换成了$5 \\times 5$卷积\n",
    "4. 和1类似，但卷积前用了最大池化层\n",
    "\n",
    "最后将这四个并行线路的结果在通道这个维度上合并在一起。\n",
    "\n",
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 256, 64, 64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incp = Inception(64, 96, 128, 16, 32, 32)\n",
    "incp.initialize()\n",
    "\n",
    "x = nd.random.uniform(shape=(32,3,64,64))\n",
    "incp(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义GoogLeNet\n",
    "\n",
    "GoogLeNet将数个Inception串联在一起。注意到原论文里使用了多个输出，为了简化我们这里就使用一个输出。为了可以更方便的查看数据在内部的形状变化，我们对每个块使用一个`nn.Sequential`，然后再把所有这些块连起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Block):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(GoogLeNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        # add name_scope on the outer most Sequential\n",
    "        with self.name_scope():\n",
    "            # block 1\n",
    "            b1 = nn.Sequential()\n",
    "            b1.add(\n",
    "                nn.Conv2D(64, kernel_size=7, strides=2,\n",
    "                          padding=3, activation='relu'),\n",
    "                nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "            # block 2\n",
    "            b2 = nn.Sequential()\n",
    "            b2.add(\n",
    "                nn.Conv2D(64, kernel_size=1),\n",
    "                nn.Conv2D(192, kernel_size=3, padding=1),\n",
    "                nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "\n",
    "            # block 3\n",
    "            b3 = nn.Sequential()\n",
    "            b3.add(\n",
    "                Inception(64, 96, 128, 16,32, 32),\n",
    "                Inception(128, 128, 192, 32, 96, 64),\n",
    "                nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "\n",
    "            # block 4\n",
    "            b4 = nn.Sequential()\n",
    "            b4.add(\n",
    "                Inception(192, 96, 208, 16, 48, 64),\n",
    "                Inception(160, 112, 224, 24, 64, 64),\n",
    "                Inception(128, 128, 256, 24, 64, 64),\n",
    "                Inception(112, 144, 288, 32, 64, 64),\n",
    "                Inception(256, 160, 320, 32, 128, 128),\n",
    "                nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "\n",
    "            # block 5\n",
    "            b5 = nn.Sequential()\n",
    "            b5.add(\n",
    "                Inception(256, 160, 320, 32, 128, 128),\n",
    "                Inception(384, 192, 384, 48, 128, 128),\n",
    "                nn.AvgPool2D(pool_size=2)\n",
    "            )\n",
    "            # block 6\n",
    "            b6 = nn.Sequential()\n",
    "            b6.add(\n",
    "                nn.Flatten(),\n",
    "                nn.Dense(num_classes)\n",
    "            )\n",
    "            # chain blocks together\n",
    "            self.net = nn.Sequential()\n",
    "            self.net.add(b1, b2, b3, b4, b5, b6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print('Block %d output: %s'%(i+1, out.shape))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看一下每个块对输出的改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 1 output: (4, 64, 23, 23)\n",
      "Block 2 output: (4, 192, 11, 11)\n",
      "Block 3 output: (4, 480, 5, 5)\n",
      "Block 4 output: (4, 832, 2, 2)\n",
      "Block 5 output: (4, 1024, 1, 1)\n",
      "Block 6 output: (4, 10)\n"
     ]
    }
   ],
   "source": [
    "net = GoogLeNet(10, verbose=True)\n",
    "net.initialize()\n",
    "\n",
    "x = nd.random.uniform(shape=(4, 3, 96, 96))\n",
    "y = net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据并训练\n",
    "\n",
    "跟VGG一样我们使用了较小的输入$96\\times 96$来加速计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  gpu(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.289, Train acc 0.27, Test acc 0.39, Time 86.8 sec\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "from mxnet import gluon\n",
    "from mxnet import init\n",
    "\n",
    "train_data, test_data = utils.load_data_fashion_mnist(\n",
    "    batch_size=64, resize=96)\n",
    "\n",
    "ctx = utils.try_gpu()\n",
    "net = GoogLeNet(10)\n",
    "net.initialize(ctx=ctx, init=init.Xavier())\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        'sgd', {'learning_rate': 0.01})\n",
    "utils.train(train_data, test_data, net, loss,\n",
    "            trainer, ctx, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "GoogLeNet加入了更加结构化的Inception块来使得我们可以使用更大的通道，更多的层，同时控制计算量和模型大小在合理范围内。\n",
    "\n",
    "## 练习\n",
    "\n",
    "GoogLeNet有数个后续版本，尝试实现他们并运行看看有什么不一样\n",
    "\n",
    "- v1: 本节介绍的是最早版本：[Going Deeper with Convolutions](http://arxiv.org/abs/1409.4842)\n",
    "- v2: 加入和Batch Normalization：[Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/abs/1502.03167)\n",
    "- v3: 对Inception做了调整：[Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)\n",
    "- v4: 基于ResNet加入了Residual Connections：[Inception-ResNet and the Impact of Residual Connections on Learning](http://arxiv.org/abs/1602.07261)\n",
    "\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/1662)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}