{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化模型参数\n",
    "\n",
    "我们仍然用MLP这个例子来详细解释如何初始化模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "46"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "\n",
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(4, activation=\"relu\"))\n",
    "        net.add(nn.Dense(2))\n",
    "    return net\n",
    "\n",
    "x = nd.random.uniform(shape=(3,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道如果不`initialize()`直接跑forward，那么系统会抱怨说参数没有初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "33"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter sequential0_dense0_weight has not been initialized. Note that you should initialize parameters and create Trainer with Block.collect_params() instead of Block.params because the later does not include Parameters of nested child Blocks"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "try:\n",
    "    net = get_net()\n",
    "    net(x)\n",
    "except RuntimeError as err:\n",
    "    sys.stderr.write(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正确的打开方式是这样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "34"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.00212593  0.00365805]\n",
       " [ 0.00161272  0.00441845]\n",
       " [ 0.00204872  0.00352518]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 访问模型参数\n",
    "\n",
    "之前我们提到过可以通过`weight`和`bias`访问`Dense`的参数，他们是`Parameter`这个类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "35"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  sequential0_dense0 \n",
      "weight:  Parameter sequential0_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>) \n",
      "bias:  Parameter sequential0_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight\n",
    "b = net[0].bias\n",
    "print('name: ', net[0].name, '\\nweight: ', w, '\\nbias: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以通过`data`来访问参数，`grad`来访问对应的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "43"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: \n",
      "[[-0.06206018  0.06491279 -0.03182812 -0.01631819 -0.00312688]\n",
      " [ 0.0408415   0.04370362  0.00404529 -0.0028032   0.00952624]\n",
      " [-0.01501013  0.05958354  0.04705103 -0.06005495 -0.02276454]\n",
      " [-0.0578019   0.02074406 -0.06716943 -0.01844618  0.04656678]]\n",
      "<NDArray 4x5 @cpu(0)>\n",
      "weight gradient \n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "<NDArray 4x5 @cpu(0)>\n",
      "bias: \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "bias gradient \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('weight:', w.data())\n",
    "print('weight gradient', w.grad())\n",
    "print('bias:', b.data())\n",
    "print('bias gradient', b.grad())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以通过`collect_params`来访问Block里面所有的参数（这个会包括所有的子Block）。它会返回一个名字到对应Parameter的dict。既可以用正常`[]`来访问参数，也可以用`get()`，它不需要填写名字的前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential0_ (\n",
      "  Parameter sequential0_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential0_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential0_dense1_weight (shape=(2, 4), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential0_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
      ")\n",
      "\n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[[-0.06206018  0.06491279 -0.03182812 -0.01631819 -0.00312688]\n",
      " [ 0.0408415   0.04370362  0.00404529 -0.0028032   0.00952624]\n",
      " [-0.01501013  0.05958354  0.04705103 -0.06005495 -0.02276454]\n",
      " [-0.0578019   0.02074406 -0.06716943 -0.01844618  0.04656678]]\n",
      "<NDArray 4x5 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params = net.collect_params()\n",
    "print(params)\n",
    "print(params['sequential0_dense0_bias'].data())\n",
    "print(params.get('dense0_weight').data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用不同的初始函数来初始化\n",
    "\n",
    "我们一直在使用默认的`initialize`来初始化权重（除了指定GPU `ctx`外）。它会把所有权重初始化成在`[-0.07, 0.07]`之间均匀分布的随机数。我们可以使用别的初始化方法。例如使用均值为0，方差为0.02的正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.00359026  0.0302582  -0.01496244  0.01725933 -0.02177767]\n",
      " [ 0.01344385  0.00272668 -0.00392631 -0.03435376  0.01124353]\n",
      " [-0.00622001  0.00689361  0.02062465  0.00675439  0.01104854]\n",
      " [ 0.01147354  0.00579418 -0.04144352 -0.02262641  0.00582818]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import init\n",
    "params.initialize(init=init.Normal(sigma=0.02), force_reinit=True)\n",
    "print(net[0].weight.data(), net[0].bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看得更加清楚点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params.initialize(init=init.One(), force_reinit=True)\n",
    "print(net[0].weight.data(), net[0].bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多的方法参见[init的API](https://mxnet.incubator.apache.org/api/python/optimization.html#the-mxnet-initializer-package). \n",
    "\n",
    "## 延后的初始化\n",
    "\n",
    "我们之前提到过Gluon的一个便利的地方是模型定义的时候不需要指定输入的大小，在之后做forward的时候会自动推测参数的大小。我们具体来看这是怎么工作的。\n",
    "\n",
    "新创建一个网络，然后打印参数。你会发现两个全连接层的权重的形状里都有0。 这是因为在不知道输入数据的情况下，我们无法判断它们的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential1_ (\n",
       "  Parameter sequential1_dense0_weight (shape=(4, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential1_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential1_dense1_weight (shape=(2, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential1_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = get_net()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential1_ (\n",
       "  Parameter sequential1_dense0_weight (shape=(4, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential1_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential1_dense1_weight (shape=(2, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential1_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会看到我们形状并没有发生变化，这是因为我们仍然不能确定权重形状。真正的初始化发生在我们看到数据时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential1_ (\n",
       "  Parameter sequential1_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential1_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential1_dense1_weight (shape=(2, 4), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential1_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候我们看到shape里面的0被填上正确的值了。\n",
    "\n",
    "## 共享模型参数\n",
    "\n",
    "有时候我们想在层之间共享同一份参数，我们可以通过Block的`params`输出参数来手动指定参数，而不是让系统自动生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(4, activation=\"relu\"))\n",
    "    net.add(nn.Dense(4, activation=\"relu\"))\n",
    "    net.add(nn.Dense(4, activation=\"relu\", params=net[-1].params))\n",
    "    net.add(nn.Dense(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化然后打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.00816047 -0.03040703  0.06714214 -0.05317248]\n",
      " [-0.01967777 -0.02854037 -0.00267491 -0.05337812]\n",
      " [ 0.02641256 -0.02548236  0.05326662 -0.01200318]\n",
      " [ 0.05855297 -0.06101935 -0.0396449   0.0269461 ]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "\n",
      "[[-0.00816047 -0.03040703  0.06714214 -0.05317248]\n",
      " [-0.01967777 -0.02854037 -0.00267491 -0.05337812]\n",
      " [ 0.02641256 -0.02548236  0.05326662 -0.01200318]\n",
      " [ 0.05855297 -0.06101935 -0.0396449   0.0269461 ]]\n",
      "<NDArray 4x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net.initialize()\n",
    "net(x)\n",
    "print(net[1].weight.data())\n",
    "print(net[2].weight.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义初始化方法\n",
    "\n",
    "下面我们自定义一个初始化方法。它通过重载`_init_weight`来实现不同的初始化方法。（注意到Gluon里面`bias`都是默认初始化成0）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight (4, 5)\n",
      "init weight (2, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 9.60578823  7.87973261  5.41556263  9.64648056  6.38859272]\n",
       " [ 6.59284496  5.04678345  8.33705139  9.21170998  5.65898943]\n",
       " [ 8.23587036  8.58163643  9.20693016  6.44703054  6.32365084]\n",
       " [ 5.91595697  6.98910379  7.93256474  7.76410723  5.10053778]]\n",
       "<NDArray 4x5 @cpu(0)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def __init__(self):\n",
    "        super(MyInit, self).__init__()\n",
    "        self._verbose = True\n",
    "    def _init_weight(self, _, arr):\n",
    "        # 初始化权重，使用out=arr后我们不需指定形状\n",
    "        print('init weight', arr.shape)\n",
    "        nd.random.uniform(low=5, high=10, out=arr)\n",
    "\n",
    "net = get_net()\n",
    "net.initialize(MyInit())\n",
    "net(x)\n",
    "net[0].weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然我们也可以通过`Parameter.set_data`来直接改写权重。注意到由于有延后初始化，所以我们通常可以通过调用一次`net(x)`来确定权重的形状先。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default weight: \n",
      "[[ 0.06699993  0.0279271  -0.05373173 -0.02835883]\n",
      " [ 0.03738332  0.0439317  -0.01234518 -0.0144892 ]]\n",
      "<NDArray 2x4 @cpu(0)>\n",
      "init to all 1s: \n",
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]]\n",
      "<NDArray 2x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net = get_net()\n",
    "net.initialize()\n",
    "net(x)\n",
    "\n",
    "print('default weight:', net[1].weight.data())\n",
    "\n",
    "w = net[1].weight\n",
    "w.set_data(nd.ones(w.shape))\n",
    "\n",
    "print('init to all 1s:', net[1].weight.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "我们可以很灵活地访问和修改模型参数。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 研究下`net.collect_params()`返回的是什么？`net.params`呢？\n",
    "1. 如何对每个层使用不同的初始化函数\n",
    "1. 如果两个层共用一个参数，那么求梯度的时候会发生什么？\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/987)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}