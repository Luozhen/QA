{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 门控循环单元（GRU）--- 从0开始\n",
    "\n",
    "[上一节](bptt.md)中，我们介绍了循环神经网络中的梯度计算方法。我们发现，循环神经网络的隐含层变量梯度可能会出现衰减或爆炸。虽然[梯度裁剪](rnn-scratch.md)可以应对梯度爆炸，但无法解决梯度衰减的问题。因此，给定一个时间序列，例如文本序列，循环神经网络在实际中其实较难捕捉两个时刻距离较大的文本元素（字或词）之间的依赖关系。\n",
    "\n",
    "门控循环神经网络（gated recurrent neural networks）的提出，是为了更好地捕捉时序数据中间隔较大的依赖关系。其中，门控循环单元（gated recurrent unit，简称GRU）是一种常用的门控循环神经网络。它由Cho、van Merrienboer、 Bahdanau和Bengio在2014年被提出。\n",
    "\n",
    "\n",
    "## 门控循环单元\n",
    "\n",
    "我们先介绍门控循环单元的构造。它比循环神经网络中的隐含层构造稍复杂一点。\n",
    "\n",
    "### 重置门和更新门\n",
    "\n",
    "门控循环单元的隐含状态只包含隐含层变量$\\mathbf{H}$。假定隐含状态长度为$h$，给定时刻$t$的一个样本数为$n$特征向量维度为$x$的批量数据$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$和上一时刻隐含状态$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$，重置门（reset gate）$\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}$和更新门（update gate）$\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}$的定义如下：\n",
    "\n",
    "$$\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r)$$\n",
    "\n",
    "$$\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z)$$\n",
    "\n",
    "其中的$\\mathbf{W}_{xr}, \\mathbf{W}_{xz} \\in \\mathbb{R}^{x \\times h}$和$\\mathbf{W}_{hr}, \\mathbf{W}_{hz} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_r, \\mathbf{b}_z \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。函数$\\sigma$自变量中的三项相加使用了[广播](../chapter_crashcourse/ndarray.md)。\n",
    "\n",
    "需要注意的是，重置门和更新门使用了值域为$[0, 1]$的函数$\\sigma(x) = 1/(1+\\text{exp}(-x))$。因此，重置门$\\mathbf{R}_t$和更新门$\\mathbf{Z}_t$中每个元素的值域都是$[0, 1]$。\n",
    "\n",
    "\n",
    "### 候选隐含状态\n",
    "\n",
    "我们可以通过元素值域在$[0, 1]$的更新门和重置门来控制隐含状态中信息的流动：这通常可以应用按元素乘法符$\\odot$。门控循环单元中的候选隐含状态$\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$使用了值域在$[-1, 1]$的双曲正切函数tanh做激活函数：\n",
    "\n",
    "$$\\tilde{\\mathbf{H}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{R}_t \\odot \\mathbf{H}_{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h)$$\n",
    "\n",
    "其中的$\\mathbf{W}_{xh} \\in \\mathbb{R}^{x \\times h}$和$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。\n",
    "\n",
    "需要注意的是，候选隐含状态使用了重置门来控制包含过去时刻信息的上一个隐含状态的流入。如果重置门近似0，上一个隐含状态将被丢弃。因此，重置门提供了丢弃与未来无关的过去隐含状态的机制。\n",
    "\n",
    "\n",
    "### 隐含状态\n",
    "\n",
    "隐含状态$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$的计算使用更新门$\\mathbf{Z}_t$来对上一时刻的隐含状态$\\mathbf{H}_{t-1}$和当前时刻的候选隐含状态$\\tilde{\\mathbf{H}}_t$做组合，公式如下：\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t$$\n",
    "\n",
    "需要注意的是，更新门可以控制过去的隐含状态在当前时刻的重要性。如果更新门一直近似1，过去的隐含状态将一直通过时间保存并传递至当前时刻。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时序数据中间隔较大的依赖关系。\n",
    "\n",
    "我们对门控循环单元的设计稍作总结：\n",
    "\n",
    "* 重置门有助于捕捉时序数据中短期的依赖关系。\n",
    "* 更新门有助于捕捉时序数据中长期的依赖关系。\n",
    "\n",
    "\n",
    "输出层的设计可参照[循环神经网络](rnn-scratch.md)中的描述。\n",
    "\n",
    "\n",
    "## 实验\n",
    "\n",
    "\n",
    "为了实现并展示门控循环单元，我们依然使用周杰伦歌词数据集来训练模型作词。这里除门控循环单元以外的实现已在[循环神经网络](rnn-scratch.md)中介绍。\n",
    "\n",
    "\n",
    "### 数据处理\n",
    "\n",
    "我们先读取并对数据集做简单处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 1465\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip', 'r') as zin:\n",
    "    zin.extractall('../data/')\n",
    "\n",
    "with open('../data/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars = f.read()\n",
    "\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:20000]\n",
    "\n",
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "print('vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用onehot来将字符索引表示成向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(data):\n",
    "    return [nd.one_hot(X, vocab_size) for X in data.T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数\n",
    "\n",
    "以下部分对模型参数进行初始化。参数`hidden_dim`定义了隐含状态的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use gpu(0)\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "# 尝试使用GPU\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from mxnet import nd\n",
    "import utils\n",
    "ctx = utils.try_gpu()\n",
    "print('Will use', ctx)\n",
    "\n",
    "input_dim = vocab_size\n",
    "# 隐含状态长度\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "std = .01\n",
    "\n",
    "def get_params():\n",
    "    # 隐含层\n",
    "    W_xz = nd.random_normal(scale=std, shape=(input_dim, hidden_dim), ctx=ctx)\n",
    "    W_hz = nd.random_normal(scale=std, shape=(hidden_dim, hidden_dim), ctx=ctx)\n",
    "    b_z = nd.zeros(hidden_dim, ctx=ctx)\n",
    "    \n",
    "    W_xr = nd.random_normal(scale=std, shape=(input_dim, hidden_dim), ctx=ctx)\n",
    "    W_hr = nd.random_normal(scale=std, shape=(hidden_dim, hidden_dim), ctx=ctx)\n",
    "    b_r = nd.zeros(hidden_dim, ctx=ctx)\n",
    "\n",
    "    W_xh = nd.random_normal(scale=std, shape=(input_dim, hidden_dim), ctx=ctx)\n",
    "    W_hh = nd.random_normal(scale=std, shape=(hidden_dim, hidden_dim), ctx=ctx)\n",
    "    b_h = nd.zeros(hidden_dim, ctx=ctx)\n",
    "\n",
    "    # 输出层\n",
    "    W_hy = nd.random_normal(scale=std, shape=(hidden_dim, output_dim), ctx=ctx)\n",
    "    b_y = nd.zeros(output_dim, ctx=ctx)\n",
    "\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hy, b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "我们将前面的模型公式翻译成代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gru_rnn(inputs, H, *params):\n",
    "    # inputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵\n",
    "    # H: 尺寸为 batch_size * hidden_dim 矩阵\n",
    "    # outputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hy, b_y = params\n",
    "    outputs = []\n",
    "    for X in inputs:        \n",
    "        Z = nd.sigmoid(nd.dot(X, W_xz) + nd.dot(H, W_hz) + b_z)\n",
    "        R = nd.sigmoid(nd.dot(X, W_xr) + nd.dot(H, W_hr) + b_r)\n",
    "        H_tilda = nd.tanh(nd.dot(X, W_xh) + R * nd.dot(H, W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = nd.dot(H, W_hy) + b_y\n",
    "        outputs.append(Y)\n",
    "    return (outputs, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "下面我们开始训练模型。我们假定谱写歌词的前缀分别为“分开”、“不分开”和“战争中部队”。这里采用的是相邻批量采样实验门控循环单元谱写歌词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Training perplexity 273.276680\n",
      " -  分开 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我\n",
      " -  不分开 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我\n",
      " -  战争中部队 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我的我的 我不的我\n",
      "\n",
      "Epoch 40. Training perplexity 105.379563\n",
      " -  分开 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你\n",
      " -  不分开 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你的爱人 我想要你\n",
      " -  战争中部队 我想你的爱我有可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱\n",
      "\n",
      "Epoch 60. Training perplexity 28.908552\n",
      " -  分开 一直一直气 配哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截\n",
      " -  不分开 不知不觉 你已经在我每了你 我想要你 我不能再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要\n",
      " -  战争中部队 我想像一步蜂天 化身为我 全怪我 的灵魂 翻滚 停i xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi x\n",
      "\n",
      "Epoch 80. Training perplexity 8.069548\n",
      " -  分开了这样就能在小里 为什么这样子 你看着我 说你有 是我的眼你每着你 看不住 说你开的想快 你 想想你的想快 我不想再你 我不要再想活 我不 我不 我不想 想不要的想快 我不想再你 我不要再想活 我不 我\n",
      " -  不分开 你说我说你 一场一直三步四望 连成线背著背默默默下心愿 看远方的星天 听说名和利都不要 陆羽泡的茶 你说会感利更加沮丧) (难道这不是我要的天堂景象 沉沦假象 你只会感到更加沮丧) (难道这不是我要的\n",
      " -  战争中部队堂 印象中的茶 听一种味道叫做家 陆羽泡的茶 听说名和利都不拿 他牵着一匹瘦马在走在涯 爷爷泡的茶 有一种味道叫做家 陆羽泡的茶 听说名和利都不拿 他牵着一匹瘦马在走在涯 爷爷泡的茶 有一种味道叫做家 \n",
      "\n",
      "Epoch 100. Training perplexity 3.143279\n",
      " -  分开了这样就能暗里 这里瓜有一只秀逗 猎物死了天边 我都有你慢 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处可躲 我不要再想 我不要再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受\n",
      " -  不分开你的手道 沉沦假象 你只会感到更加沮丧) (难道这不是我要的天堂景象 沉沦假象 你只会感到更加沮丧) (难道这不是我要的天堂景象 沉沦假象 你只会感到更加沮丧) (难道这不是我要的天堂景象 沉沦假象 你\n",
      " -  战争中部队堂) (袋瓜　不是我要的天堂景象 沉沦假象 你只会感到更加沮丧) (难道这不是我要的天堂景象 沉沦假象 你只会感到更加沮丧) (难道这不是我要的天堂景象 沉沦假象 你只会感到更加沮丧) (难道这不是我要\n",
      "\n",
      "Epoch 120. Training perplexity 1.707154\n",
      " -  分开了这一场戏戏就习活 为什么我妈能说 一直悲剧 悲伤了真的没有一口 让牵有一只子就法的凯萨 传古主明只剩下难解的语言 传说就成了永垂不朽的诗篇 我给你的爱写在西元前 深埋在美索不达米亚平原 用楔形文字刻下\n",
      " -  不分开你的寂道 让你叫美索不再说 我不能再想 我不能再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处可躲 我不要再想 我不要再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处\n",
      " -  战争中部队着你 趁时间没讯号 让我带着你 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂\n",
      "\n",
      "Epoch 140. Training perplexity 1.237012\n",
      " -  分开了这一身戏 全感道过去 我摇往醒抽离开开江 为人翘没发觉 让我带着你离开 这不是顽固 这不是逃避 没人帮着你走才快乐 没人帮着你走才快乐 我不能再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受\n",
      " -  不分开你的难道 让你放美 简有多间已慢不要要你 爸笑的课息 我很悔感忆 (没有你烦我有多烦恼多难熬) 穿过云层不再回到你看到 但偏偏当渐在流血我 你的想笑已狂 那什么我连爱你 蜗牛 -<简！夫a 我的天 败给\n",
      " -  战争中部队着) 想回到过去 沉默支撑跃过 我向西引北风 晒成一身古铜 渴望着血脉相通 无限个千万弟兄 我把天地拆封将长江水掏空 人在古老河床蜕变中 我右拳打开了天 化身为龙 把山河重新移动 填平裂缝 将东方 的日\n",
      "\n",
      "Epoch 160. Training perplexity 1.099784\n",
      " -  分开了这样 放在小愿静 半茶之人顶峰 我向西引北风 晒成一身古铜 渴望着血脉相通 无限个千万弟兄 我把天地拆封将长江水掏空 人在古老河床蜕变中 我右拳打开了天长 爱你的爱你在你 不想再这样打我妈妈 难道你手\n",
      " -  不分开就走 把手慢慢交给我 放下心中的困惑 雨点从两旁划过 割开两种精神的我 经过老伯的家 篮框变得好高 爬过的那棵树 又何时变得渺小 这样也好 开始没人注意到我 等雨变强之前 我们将会分化软弱 趁时间没发觉\n",
      " -  战争中部队着) 想回到过去 试着抱你在怀里 羞怯的脸带有一点稚气 想看你看的世界 想在你梦的画面 只要靠在一起就能感觉甜蜜 想回到过去 试着让故事继续 至少不再让你离我而去 分散时间的注意 这次会抱得更紧 这样挽\n",
      "\n",
      "Epoch 180. Training perplexity 1.064461\n",
      " -  分开了这样 放过 但是连咪都ㄟ 但是连咪都ㄟ 过去按怎你走那个 当时 过去按怎你走那个 当时 没留半张批纸 没留半张批纸 加字过去 像溪边ㄟ田蝇 龙是 逗阵飞ㄟ日子 龙是 逗阵飞ㄟ日子 这时 我想离水ㄟ鱼 \n",
      " -  不分开就走 把手慢慢交给我 放下心中的困惑 雨点从两旁划过 割开两种精神的我 经过老伯的家 篮框变得好高 爬过的那棵树 又何时变得渺小 这样也好 开始没人注意到我 等雨变强之前 我们将会分化软弱 趁时间没发觉\n",
      " -  战争中部队着 它一定我想绕的乾净 一点又剧演划过的沼面 铺著榉木板的屋内还弥漫 姥姥当年酿的豆瓣酱 我对著黑白照片开始想像 爸和妈当年的模样 说著一口吴侬软语的姑娘缓缓走过外滩 消失的 旧时光 一九四三 在回忆 \n",
      "\n",
      "Epoch 200. Training perplexity 1.051433\n",
      " -  分开了暗 还真在笑险鼻缘 我们耍的风界将默都不想 我爱你 爱情我 说你怎么面对我 不知不觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活 不知不觉 你已\n",
      " -  不分开就走 把手慢慢交给我 放下心中的困惑 雨点从两旁划过 割开两种精神的我 经过老伯的家 篮框变得好高 爬过的那棵树 又何时变得渺小 这样也好 开始没人注意到我 等雨变强之前 我们将会分化软弱 趁时间没发觉\n",
      " -  战争中部队着旁它的窝 它在灌木的那是我记记 我一定老呵照三 他看着我不微笑笑 我不想再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处可躲 我不要再想 我不要再想 我不 我不 我不要再想你 爱情来\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq1 = '分开'\n",
    "seq2 = '不分开'\n",
    "seq3 = '战争中部队'\n",
    "seqs = [seq1, seq2, seq3]\n",
    "\n",
    "utils.train_and_predict_rnn(rnn=gru_rnn, is_random_iter=False, epochs=200,\n",
    "                            num_steps=35, hidden_dim=hidden_dim, \n",
    "                            learning_rate=0.2, clipping_norm=5,\n",
    "                            batch_size=32, pred_period=20, pred_len=100,\n",
    "                            seqs=seqs, get_params=get_params,\n",
    "                            get_inputs=get_inputs, ctx=ctx,\n",
    "                            corpus_indices=corpus_indices,\n",
    "                            idx_to_char=idx_to_char, char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到一开始学到简单的字符，然后简单的词，接着是复杂点的词，然后看上去似乎像个句子了。\n",
    "\n",
    "## 结论\n",
    "\n",
    "* 门控循环单元的提出是为了更好地捕捉时序数据中间隔较大的依赖关系。\n",
    "* 重置门有助于捕捉时序数据中短期的依赖关系。\n",
    "* 更新门有助于捕捉时序数据中长期的依赖关系。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 调调参数（例如数据集大小、序列长度、隐含状态长度和学习率），看看对运行时间、perplexity和预测的结果造成的影响。\n",
    "* 在相同条件下，比较门控循环单元和循环神经网络的运行效率。\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/4042)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
