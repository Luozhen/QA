{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络 --- 从0开始\n",
    "\n",
    "前面的教程里我们使用的网络都属于**前馈神经网络**。之所以叫前馈，是因为整个网络是一条链（回想下`gluon.nn.Sequential`），每一层的结果都是反馈给下一层。这一节我们介绍**循环神经网络**，这里每一层不仅输出给下一层，同时还输出一个**隐含状态**，给当前层在处理下一个样本时使用。下图展示这两种网络的区别。\n",
    "\n",
    "![](../img/rnn_1.png)\n",
    "\n",
    "循环神经网络的这种结构使得它适合处理前后有依赖关系数据样本。我们拿语言模型举个例子来解释这个是怎么工作的。语言模型的任务是给定句子的前*t*个字符，然后预测第*t+1*个字符。假设我们的句子是“你好世界”，使用前馈神经网络来预测的一个做法是，在时间1输入“你”，预测”好“，时间2向同一个网络输入“好”预测“世”。下图左边展示了这个过程。\n",
    "\n",
    "![](../img/rnn_2.png)\n",
    "\n",
    "注意到一个问题是，当我们预测“世”的时候只给了“好”这个输入，而完全忽略了“你”。直觉上“你”这个词应该对这次的预测比较重要。虽然这个问题通常可以通过**n-gram**来缓解，就是说预测第*t+1*个字符的时候，我们输入前*n*个字符。如果*n=1*，那就是我们这里用的。我们可以增大*n*来使得输入含有更多信息。但我们不能任意增大*n*，因为这样通常带来模型复杂度的增加从而导致需要大量数据和计算来训练模型。\n",
    "\n",
    "循环神经网络使用一个隐含状态来记录前面看到的数据来帮助当前预测。上图右边展示了这个过程。在预测“好”的时候，我们输出一个隐含状态。我们用这个状态和新的输入“好”来一起预测“世”，然后同时输出一个更新过的隐含状态。我们希望前面的信息能够保存在这个隐含状态里，从而提升预测效果。\n",
    "\n",
    "## 循环神经网络\n",
    "\n",
    "在对输入输出数据有了解后，我们来正式介绍循环神经网络。\n",
    "\n",
    "首先回忆一下单隐含层的前馈神经网络的定义，例如[多层感知机](../chapter_supervised-learning/mlp-scratch.md)。假设隐含层的激活函数是$\\phi$，对于一个样本数为$n$特征向量维度为$x$的批量数据$\\mathbf{X} \\in \\mathbb{R}^{n \\times x}$（$\\mathbf{X}$是一个$n$行$x$列的实数矩阵）来说，那么这个隐含层的输出就是\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h)$$\n",
    "\n",
    "假定隐含层长度为$h$，其中的$\\mathbf{W}_{xh} \\in \\mathbb{R}^{x \\times h}$是权重参数。偏移参数 $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$在与前一项$\\mathbf{X} \\mathbf{W}_{xh} \\in \\mathbb{R}^{n \\times h}$ 相加时使用了[广播](../chapter_crashcourse/ndarray.md)。这个隐含层的输出的尺寸为$\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$。\n",
    "\n",
    "把隐含层的输出$\\mathbf{H}$作为输出层的输入，最终的输出\n",
    "\n",
    "$$\\hat{\\mathbf{Y}} = \\text{softmax}(\\mathbf{H} \\mathbf{W}_{hy} + \\mathbf{b}_y)$$\n",
    "\n",
    "假定每个样本对应的输出向量维度为$y$，其中 $\\hat{\\mathbf{Y}} \\in \\mathbb{R}^{n \\times y}, \\mathbf{W}_{hy} \\in \\mathbb{R}^{h \\times y}, \\mathbf{b}_y \\in \\mathbb{R}^{1 \\times y}$且两项相加使用了[广播](../chapter_crashcourse/ndarray.md)。\n",
    "\n",
    "\n",
    "将上面网络改成循环神经网络，我们首先对输入输出加上时间戳$t$。假设$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$是序列中的第$t$个批量输入（样本数为$n$，每个样本的特征向量维度为$x$），对应的隐含层输出是隐含状态$\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$（隐含层长度为$h$），而对应的最终输出是$\\hat{\\mathbf{Y}}_t \\in \\mathbb{R}^{n \\times y}$（每个样本对应的输出向量维度为$y$）。在计算隐含层的输出的时候，循环神经网络只需要在前馈神经网络基础上加上跟前一时间$t-1$输入隐含层$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$的加权和。为此，我们引入一个新的可学习的权重$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$：\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h)$$\n",
    "\n",
    "输出的计算跟前面一致：\n",
    "\n",
    "$$\\hat{\\mathbf{Y}}_t = \\text{softmax}(\\mathbf{H}_t \\mathbf{W}_{hy}  + \\mathbf{b}_y)$$\n",
    "\n",
    "一开始我们提到过，隐含状态可以认为是这个网络的记忆。该网络中，时刻$t$的隐含状态就是该时刻的隐含层变量$\\mathbf{H}_t$。它存储前面时间里面的信息。我们的输出是只基于这个状态。最开始的隐含状态里的元素通常会被初始化为0。\n",
    "\n",
    "\n",
    "## 周杰伦歌词数据集\n",
    "\n",
    "\n",
    "为了实现并展示循环神经网络，我们使用周杰伦歌词数据集来训练模型作词。该数据集里包含了著名创作型歌手周杰伦从第一张专辑《Jay》到第十张专辑《跨时代》所有歌曲的歌词。\n",
    "\n",
    "![](../img/jay.jpg)\n",
    "\n",
    "\n",
    "下面我们读取这个数据并看看前面49个字符（char）是什么样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每天在想想想想著你\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip', 'r') as zin:\n",
    "    zin.extractall('../data/')\n",
    "\n",
    "with open('../data/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars = f.read()\n",
    "print(corpus_chars[0:49])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看一下数据集里的字符数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64925"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们稍微处理下数据集。为了打印方便，我们把换行符替换成空格，然后截去后面一段使得接下来的训练会快一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字符的数值表示\n",
    "\n",
    "先把数据里面所有不同的字符拿出来做成一个字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 1465\n"
     ]
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "print('vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后可以把每个字符转成从0开始的索引(index)来方便之后的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: \n",
      " 想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每\n",
      "\n",
      "indices: \n",
      " [175, 859, 1213, 310, 352, 231, 535, 175, 859, 437, 1286, 822, 576, 384, 507, 895, 535, 175, 859, 437, 1286, 1030, 376, 374, 375, 188, 535, 1030, 376, 374, 384, 507, 723, 535, 509, 207, 885, 207, 885, 207]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "\n",
    "sample = corpus_indices[:40]\n",
    "\n",
    "print('chars: \\n', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('\\nindices: \\n', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时序数据的批量采样\n",
    "\n",
    "同之前一样我们需要每次随机读取一些（`batch_size`个）样本和其对用的标号。这里的样本跟前面有点不一样，这里一个样本通常包含一系列连续的字符（前馈神经网络里可能每个字符作为一个样本）。\n",
    "\n",
    "如果我们把序列长度（`num_steps`）设成5，那么一个可能的样本是“想要有直升”。其对应的标号仍然是长为5的序列，每个字符是对应的样本里字符的后面那个。例如前面样本的标号就是“要有直升机”。\n",
    "\n",
    "\n",
    "### 随机批量采样\n",
    "\n",
    "下面代码每次从数据里随机采样一个批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from mxnet import nd\n",
    "\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    # 减一是因为label的索引是相应data的索引加一\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    # 随机化样本\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    # 返回num_steps个数据\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos: pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "        data = nd.array(\n",
    "            [_data(j * num_steps) for j in batch_indices], ctx=ctx)\n",
    "        label = nd.array(\n",
    "            [_data(j * num_steps + 1) for j in batch_indices], ctx=ctx)\n",
    "        yield data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了便于理解时序数据上的随机批量采样，让我们输入一个从0到29的人工序列，看下读出来长什么样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  \n",
      "[[  3.   4.   5.]\n",
      " [ 24.  25.  26.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[  4.   5.   6.]\n",
      " [ 25.  26.  27.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[ 12.  13.  14.]\n",
      " [  9.  10.  11.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 13.  14.  15.]\n",
      " [ 10.  11.  12.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[ 0.  1.  2.]\n",
      " [ 6.  7.  8.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 1.  2.  3.]\n",
      " [ 7.  8.  9.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[ 21.  22.  23.]\n",
      " [ 18.  19.  20.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 22.  23.  24.]\n",
      " [ 19.  20.  21.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "\n",
    "for data, label in data_iter_random(my_seq, batch_size=2, num_steps=3):\n",
    "    print('data: ', data, '\\nlabel:', label, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于各个采样在原始序列上的位置是随机的时序长度为`num_steps`的连续数据点，相邻的两个随机批量在原始序列上的位置不一定相毗邻。因此，在训练模型时，读取每个随机时序批量前需要重新初始化隐含状态。\n",
    "\n",
    "\n",
    "### 相邻批量采样\n",
    "\n",
    "除了对原序列做随机批量采样之外，我们还可以使相邻的两个随机批量在原始序列上的位置相毗邻。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "\n",
    "    indices = corpus_indices[0: batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    # 减一是因为label的索引是相应data的索引加一\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        data = indices[:, i: i + num_steps]\n",
    "        label = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相同地，为了便于理解时序数据上的相邻批量采样，让我们输入一个从0到29的人工序列，看下读出来长什么样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  \n",
      "[[  0.   1.   2.]\n",
      " [ 15.  16.  17.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[  1.   2.   3.]\n",
      " [ 16.  17.  18.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[  3.   4.   5.]\n",
      " [ 18.  19.  20.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[  4.   5.   6.]\n",
      " [ 19.  20.  21.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[  6.   7.   8.]\n",
      " [ 21.  22.  23.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[  7.   8.   9.]\n",
      " [ 22.  23.  24.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[  9.  10.  11.]\n",
      " [ 24.  25.  26.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 10.  11.  12.]\n",
      " [ 25.  26.  27.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "\n",
    "for data, label in data_iter_consecutive(my_seq, batch_size=2, num_steps=3):\n",
    "    print('data: ', data, '\\nlabel:', label, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于各个采样在原始序列上的位置是毗邻的时序长度为`num_steps`的连续数据点，因此，使用相邻批量采样训练模型时，读取每个时序批量前，我们需要将该批量最开始的隐含状态设为上个批量最终输出的隐含状态。在同一个epoch中，隐含状态只需要在该epoch开始的时候初始化。\n",
    "\n",
    "\n",
    "## One-hot向量\n",
    "\n",
    "注意到每个字符现在是用一个整数来表示，而输入进网络我们需要一个定长的向量。一个常用的办法是使用one-hot来将其表示成向量。也就是说，如果一个字符的整数值是$i$, 那么我们创建一个全0的长为`vocab_size`的向量，并将其第$i$位设成1。该向量就是对原字符的one-hot向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
       " [ 0.  0.  1. ...,  0.  0.  0.]]\n",
       "<NDArray 2x1465 @cpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.one_hot(nd.array([0, 2]), vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记得前面我们每次得到的数据是一个`batch_size * num_steps`的批量。下面这个函数将其转换成`num_steps`个可以输入进网络的`batch_size * vocab_size`的矩阵。对于一个长度为`num_steps`的序列，每个批量输入$\\mathbf{X} \\in \\mathbb{R}^{n \\times x}$，其中$n=$ `batch_size`，而$x=$`vocab_size`（onehot编码向量维度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length:  3\n",
      "input[0] shape:  (2, 1465)\n"
     ]
    }
   ],
   "source": [
    "def get_inputs(data):\n",
    "    return [nd.one_hot(X, vocab_size) for X in data.T]\n",
    "\n",
    "inputs = get_inputs(data)\n",
    "\n",
    "print('input length: ', len(inputs))\n",
    "print('input[0] shape: ', inputs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "对于序列中任意一个时间戳，一个字符的输入是维度为`vocab_size`的one-hot向量，对应输出是预测下一个时间戳为词典中任意字符的概率，因而该输出是维度为`vocab_size`的向量。\n",
    "\n",
    "当序列中某一个时间戳的输入为一个样本数为`batch_size`（对应模型定义中的$n$）的批量，每个时间戳上的输入和输出皆为尺寸`batch_size * vocab_size`（对应模型定义中的$n \\times x$）的矩阵。假设每个样本对应的隐含状态的长度为`hidden_dim`（对应模型定义中隐含层长度$h$），根据矩阵乘法定义，我们可以推断出模型隐含层和输出层中各个参数的尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use gpu(0)\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "# 尝试使用GPU\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "ctx = utils.try_gpu()\n",
    "print('Will use', ctx)\n",
    "\n",
    "input_dim = vocab_size\n",
    "# 隐含状态长度\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "std = .01\n",
    "\n",
    "def get_params():\n",
    "    # 隐含层\n",
    "    W_xh = nd.random_normal(scale=std, shape=(input_dim, hidden_dim), ctx=ctx)\n",
    "    W_hh = nd.random_normal(scale=std, shape=(hidden_dim, hidden_dim), ctx=ctx)\n",
    "    b_h = nd.zeros(hidden_dim, ctx=ctx)\n",
    "\n",
    "    # 输出层\n",
    "    W_hy = nd.random_normal(scale=std, shape=(hidden_dim, output_dim), ctx=ctx)\n",
    "    b_y = nd.zeros(output_dim, ctx=ctx)\n",
    "\n",
    "    params = [W_xh, W_hh, b_h, W_hy, b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "当序列中某一个时间戳的输入为一个样本数为`batch_size`的批量，而整个序列长度为`num_steps`时，以下`rnn`函数的`inputs`和`outputs`皆为`num_steps` 个尺寸为`batch_size * vocab_size`的矩阵，隐含变量$\\mathbf{H}$是一个尺寸为`batch_size * hidden_dim`的矩阵。该隐含变量$\\mathbf{H}$也是循环神经网络的隐含状态`state`。\n",
    "\n",
    "我们将前面的模型公式翻译成代码。这里的激活函数使用了按元素操作的双曲正切函数\n",
    "\n",
    "$$\\text{tanh}(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}$$\n",
    "\n",
    "需要注意的是，双曲正切函数的值域是$[-1, 1]$。如果自变量均匀分布在整个实域，该激活函数输出的均值为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn(inputs, state, *params):\n",
    "    # inputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵。\n",
    "    # H: 尺寸为 batch_size * hidden_dim 矩阵。\n",
    "    # outputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵。\n",
    "    H = state\n",
    "    W_xh, W_hh, b_h, W_hy, b_y = params\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)\n",
    "        Y = nd.dot(H, W_hy) + b_y\n",
    "        outputs.append(Y)\n",
    "    return (outputs, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做个简单的测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length:  3\n",
      "output[0] shape:  (2, 1465)\n",
      "state shape:  (2, 256)\n"
     ]
    }
   ],
   "source": [
    "state = nd.zeros(shape=(data.shape[0], hidden_dim), ctx=ctx)\n",
    "\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(get_inputs(data.as_in_context(ctx)), state, *params)\n",
    "\n",
    "print('output length: ',len(outputs))\n",
    "print('output[0] shape: ', outputs[0].shape)\n",
    "print('state shape: ', state_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测序列\n",
    "\n",
    "在做预测时我们只需要给定时间0的输入和起始隐含变量。然后我们每次将上一个时间的输出作为下一个时间的输入。\n",
    "\n",
    "![](../img/rnn_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_rnn(rnn, prefix, num_chars, params, hidden_dim, ctx, idx_to_char,\n",
    "                char_to_idx, get_inputs, is_lstm=False):\n",
    "    # 预测以 prefix 开始的接下来的 num_chars 个字符。\n",
    "    prefix = prefix.lower()\n",
    "    state_h = nd.zeros(shape=(1, hidden_dim), ctx=ctx)\n",
    "    if is_lstm:\n",
    "        # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "        state_c = nd.zeros(shape=(1, hidden_dim), ctx=ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for i in range(num_chars + len(prefix)):\n",
    "        X = nd.array([output[-1]], ctx=ctx)\n",
    "        # 在序列中循环迭代隐含变量。\n",
    "        if is_lstm:\n",
    "            # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "            Y, state_h, state_c = rnn(get_inputs(X), state_h, state_c, *params)\n",
    "        else:\n",
    "            Y, state_h = rnn(get_inputs(X), state_h, *params)\n",
    "        if i < len(prefix)-1:\n",
    "            next_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            next_input = int(Y[0].argmax(axis=1).asscalar())\n",
    "        output.append(next_input)\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度剪裁\n",
    "\n",
    "我们在[正向传播和反向传播](../chapter_supervised-learning/backprop.md)中提到，\n",
    "训练神经网络往往需要依赖梯度计算的优化算法，例如我们之前介绍的[随机梯度下降](../chapter_supervised-learning/linear-regression-scratch.md)。\n",
    "而在循环神经网络的训练中，当每个时序训练数据样本的时序长度`num_steps`较大或者时刻$t$较小，目标函数有关$t$时刻的隐含层变量梯度较容易出现衰减（valishing）或爆炸（explosion）。我们会在[下一节](bptt.md)详细介绍出现该现象的原因。\n",
    "\n",
    "为了应对梯度爆炸，一个常用的做法是如果梯度特别大，那么就投影到一个比较小的尺度上。假设我们把所有梯度接成一个向量 $\\boldsymbol{g}$，假设剪裁的阈值是$\\theta$，那么我们这样剪裁使得$\\|\\boldsymbol{g}\\|$不会超过$\\theta$：\n",
    "\n",
    "$$ \\boldsymbol{g} = \\min\\left(\\frac{\\theta}{\\|\\boldsymbol{g}\\|}, 1\\right)\\boldsymbol{g}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, ctx):\n",
    "    if theta is not None:\n",
    "        norm = nd.array([0.0], ctx)\n",
    "        for p in params:\n",
    "            norm += nd.sum(p.grad ** 2)\n",
    "        norm = nd.sqrt(norm).asscalar()\n",
    "        if norm > theta:\n",
    "            for p in params:\n",
    "                p.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "下面我们可以还是训练模型。跟前面前置网络的教程比，这里有以下几个不同。\n",
    "\n",
    "1. 通常我们使用困惑度（Perplexity）这个指标。\n",
    "2. 在更新前我们对梯度做剪裁。\n",
    "3. 在训练模型时，对时序数据采用不同批量采样方法将导致隐含变量初始化的不同。\n",
    "\n",
    "### 困惑度（Perplexity）\n",
    "\n",
    "回忆以下我们之前介绍的[交叉熵损失函数](../chapter_supervised-learning/softmax-regression-scratch.md)。在语言模型中，该损失函数即被预测字符的对数似然平均值的相反数：\n",
    "\n",
    "$$\\text{loss} = -\\frac{1}{N} \\sum_{i=1}^N \\log p_{\\text{target}_i}$$\n",
    "\n",
    "其中$N$是预测的字符总数，$p_{\\text{target}_i}$是在第$i$个预测中真实的下个字符被预测的概率。\n",
    "\n",
    "而这里的困惑度可以简单的认为就是对交叉熵做exp运算使得数值更好读。\n",
    "\n",
    "为了解释困惑度的意义，我们先考虑一个完美结果：模型总是把真实的下个字符的概率预测为1。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 1$。这种完美情况下，困惑度值为1。\n",
    "\n",
    "我们再考虑一个基线结果：给定不重复的字符集合$W$及其字符总数$|W|$，模型总是预测下个字符为集合$W$中任一字符的概率都相同。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 1/|W|$。这种基线情况下，困惑度值为$|W|$。\n",
    "\n",
    "最后，我们可以考虑一个最坏结果：模型总是把真实的下个字符的概率预测为0。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 0$。这种最坏情况下，困惑度值为正无穷。\n",
    "\n",
    "任何一个有效模型的困惑度值必须小于预测集中元素的数量。在本例中，困惑度必须小于字典中的字符数$|W|$。如果一个模型可以取得较低的困惑度的值（更靠近1），通常情况下，该模型预测更加准确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from math import exp\n",
    "           \n",
    "def train_and_predict_rnn(rnn, is_random_iter, epochs, num_steps, hidden_dim, \n",
    "                          learning_rate, clipping_theta, batch_size,\n",
    "                          pred_period, pred_len, seqs, get_params, get_inputs,\n",
    "                          ctx, corpus_indices, idx_to_char, char_to_idx,\n",
    "                          is_lstm=False):\n",
    "    if is_random_iter:\n",
    "        data_iter = data_iter_random\n",
    "    else:\n",
    "        data_iter = data_iter_consecutive\n",
    "    params = get_params()\n",
    "    \n",
    "    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        # 如使用相邻批量采样，在同一个epoch中，隐含变量只需要在该epoch开始的时候初始化。\n",
    "        if not is_random_iter:\n",
    "            state_h = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "            if is_lstm:\n",
    "                # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "                state_c = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "        train_loss, num_examples = 0, 0\n",
    "        for data, label in data_iter(corpus_indices, batch_size, num_steps, \n",
    "                                     ctx):\n",
    "            # 如使用随机批量采样，处理每个随机小批量前都需要初始化隐含变量。\n",
    "            if is_random_iter:\n",
    "                state_h = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "                if is_lstm:\n",
    "                    # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "                    state_c = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "            with autograd.record():\n",
    "                # outputs 尺寸：(batch_size, vocab_size)\n",
    "                if is_lstm:\n",
    "                    # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "                    outputs, state_h, state_c = rnn(get_inputs(data), state_h,\n",
    "                                                    state_c, *params) \n",
    "                else:\n",
    "                    outputs, state_h = rnn(get_inputs(data), state_h, *params)\n",
    "                # 设t_ib_j为i时间批量中的j元素:\n",
    "                # label 尺寸：（batch_size * num_steps）\n",
    "                # label = [t_0b_0, t_0b_1, ..., t_1b_0, t_1b_1, ..., ]\n",
    "                label = label.T.reshape((-1,))\n",
    "                # 拼接outputs，尺寸：(batch_size * num_steps, vocab_size)。\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                # 经上述操作，outputs和label已对齐。\n",
    "                loss = softmax_cross_entropy(outputs, label)\n",
    "            loss.backward()\n",
    "\n",
    "            grad_clipping(params, clipping_theta, ctx)\n",
    "            utils.SGD(params, learning_rate)\n",
    "\n",
    "            train_loss += nd.sum(loss).asscalar()\n",
    "            num_examples += loss.size\n",
    "\n",
    "        if e % pred_period == 0:\n",
    "            print(\"Epoch %d. Perplexity %f\" % (e, \n",
    "                                               exp(train_loss/num_examples)))\n",
    "            for seq in seqs:\n",
    "                print(' - ', predict_rnn(rnn, seq, pred_len, params,\n",
    "                      hidden_dim, ctx, idx_to_char, char_to_idx, get_inputs,\n",
    "                      is_lstm))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下定义模型参数和预测序列前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "num_steps = 35\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "seq1 = '分开'\n",
    "seq2 = '不分开'\n",
    "seq3 = '战争中部队'\n",
    "seqs = [seq1, seq2, seq3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先采用随机批量采样实验循环神经网络谱写歌词。我们假定谱写歌词的前缀分别为“分开”、“不分开”和“战争中部队”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Perplexity 218.592780\n",
      " -  分开 我想的让我 我的你的茶我的可 我有的让我 我的你的茶我的可 我不的让我 我的让我想 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让\n",
      " -  不分开 我想的让我 我的让我想 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让\n",
      " -  战争中部队 我想的让我 我的你的茶我的可 我有的让我 我的你的茶我的可 我不的让我 我的让我想 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让我 我想的让\n",
      "\n",
      "Epoch 40. Perplexity 83.306413\n",
      " -  分开 我不想再你 我不要我不见 你的手不 我不要我不要 你的手不 我不要我不要 你的手不 我不要我不要 你的手不 我不要我不要 你的手不 我不要我不要 你的手不 我不要我不要 你的手不 我不要我不要 你的手\n",
      " -  不分开 我的世界 我不要 你怎么 一颗两步三颗四 看想有我的微负 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知\n",
      " -  战争中部队 我想能你的微水 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你的可知 我想想你\n",
      "\n",
      "Epoch 60. Perplexity 30.591603\n",
      " -  分开 我有一直一步 哼哼哈 的最魂 单不是我的想  让我们不了 你在我有多不 我不能 你不是 不想 有你在 木子我 别子 有一种两 温果的人 我说一起 你已的可 就是一种 我想了那 龙你的可后 然不到 瞎不\n",
      " -  不分开  在我有多打 我不能再 你我的世婆 让我们始的 你 是不些我 不知不觉 我跟了那生龙 我知不觉 你已的世我 不的表写女人 一果我遇见你 有你的些 我知了老 我要了这样龙 我知不觉 你已的世我想红的可爱\n",
      " -  战争中部队 (在你 别子我 单你 有一种两 在你在我的溪负 让我们 半兽人 的灵魂 翻滚 停起 xi xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ xㄚ x\n",
      "\n",
      "Epoch 80. Perplexity 12.773101\n",
      " -  分开 我不要再想你 不知不觉 你已经离开我 不知不觉 我跟了那条奏 后知后觉 你已经离开我 不知不觉 我跟经离开活 后知后觉 你已经离开我 不知不觉 我跟经离开活 后知后觉 你已经离开我 不知不觉 我跟经离\n",
      " -  不分开  我不能你想要 这样的证匙我 不起半对的太面 没有你在接 我感一起觉祷 你说一起一步走 我会要你已经开久 这想躲 说你眼睛看着我 说不 我很要 爱你走 别子我 说球 只单 xi xー xi xi xi\n",
      " -  战争中部队  想该伊斯坦堡 你只会不到我开不知不觉 我给你的爱写在西元前 深埋在美索不达米亚平 我不能再想 我不要再想 我不能再想 我不 我不 我不要 爱情走的太快就像龙卷风 不能承　我已无 可忆的没匙我 一起半\n",
      "\n",
      "Epoch 100. Perplexity 6.983885\n",
      " -  分开 我有一定一你没讯 别想我说你走着 但那为我连爱你 是因为我太爱你 是因不 让你走始 我想一直是你 我们不起分活 我不能再你 强不是我不 你不能再想 我不能再想 我不能再想 我不能再想 我不能再想 我不\n",
      " -  不分开  我已 你不人 一你ㄟ颗三颗四颗 连成线背著背默默许下心堂 看远方的星如否听的见 它一定美现 往平 什么再听倒的那 我说黑着 当小的在婆刻动一道孤独 你在那里 在小村外的溪边 默默等待 娘子 一壶筐 \n",
      " -  战争中部队 ( 泡过的那口 维会谅在的出  只在不里 你不会好到我已不知 不去 这样过 木炭 一直走 木炭 一直半 木炭 一直半 木炭 一直半 木炭 一直半 木炭 一直半 木炭 一直半 木炭 一直半 木炭 一直半\n",
      "\n",
      "Epoch 120. Perplexity 4.593975\n",
      " -  分开 我也世界想走 那果水的街叫我而何能够说的手口 我想你的想 我而会很我 不知说 说你已著子光 他才等老琴 我的世界 被摧是你 在一种空 我右不同我龙要了 别时走 一只两颗三颗四步 连成线背著背默默许下心\n",
      " -  不分开 已经已经看不到 不少就耳万过 我想能 你怎么 快不了 别滚 是非完 x对伦 一壶两颗三颗四颗 连成线背著背默默许下心堂 看远方的星如 听不能说你 那灵他然滴的茶 什么我的别 是一场味废动的响尾蛇 站睛\n",
      " -  战争中部队天 一双正张心回我进罩的脸 经反前会有来的事馆 波弦内林不忠 所有回忆对着我进攻 我的黑界主你没想 你怎么ㄟ用龙没讯息 我亲像打我了天 化身的耳一天跳接 一些令 被堆眼睛看着 一身时气 又安用双截棍 哼\n",
      "\n",
      "Epoch 140. Perplexity 3.395364\n",
      " -  分开了天不过 一定~~旧仪的母斑鸠 印地安的风岸在然一起风深 就所我听了难 一场走剧睡 我好的话窗 三什么断落 这不的让你 你的完美主人 太彻 却事很考倒我 说散 你想很久了吧? 我开你的爱写在西元前 深埋\n",
      " -  不分开 已经 一去B血型 恒 该对抽！ 如在晶空 恨安了空 我有没带我 不着悲陆 喝眼作过 在一种钟 没有不同 你不懂 的我出 暗不能够 一场跳龙年 等在一间 在小完中的溪边河口默默等著我 娘子依旧每沉折望 \n",
      " -  战争中部队过掏到 爷爷泡的茶 有一种味道叫做家 他满泡的茶 有一种味道叫做家 他满泡的发 有一种味道叫做家 他满泡的发 有一种味道叫做家 他满泡的发 有一种味道叫做家 他满泡的发 有一种味道叫做家 他满泡的发 有\n",
      "\n",
      "Epoch 160. Perplexity 2.766516\n",
      " -  分开我妈不难堪 我要你的爱写在西元前 深埋在美索不达米亚平原 几十个世纪后出土发现 泥已风 一点两步三颗四颗望著天 看星星 一颗两颗三颗四颗 连成线背著背默默许下心愿 看远方的星如果听 说 你在完乐 有过的\n",
      " -  不分开 已经 这去 ㄙㄡ ㄈㄚ ㄇー ㄇー ㄇㄚ ㄇー xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi x\n",
      " -  战争中部队着掏空 爷爷泡的茶 有一种味道叫做家 他满泡白发 在那时不准说不 陆羽泡的茶 像幅泼墨的山水画 唐朝千年的风沙 现在还在刮 (千年会不想不到 一只令它心仪的母斑鸠 印地一阵饿 吹也它 说在这种 啦杰的梦\n",
      "\n",
      "Epoch 180. Perplexity 2.401983\n",
      " -  分开了你不会 无心序　做你的民谣 和弦到容不忠 所有回忆对着我进攻 我的离口被你拆封 誓言 在著人甘着我 说散 你想很久了吧?  夹时间有里堡 你 想和你骑棒开 想这样没担忧 唱着歌 一直走 我想就这不得 \n",
      " -  不分开 已经 这去意义型的公 鼠对暴力忠 你的那界主义摧毁 也许颓废也是另一种美 也许颓废也是另一种美 眼著的雾猫在否的喊想 孤单的壳裹着轻轻地仰望 我轻无的心写在然清前 深埋在美索不达米亚平原 几十个世纪后\n",
      " -  战争中部队落掏也 回到承幕被风吹动 静光不言的离吻 不被污染的转生 维持纯白的象校 维后还原为人 让我们 半兽人 的灵魂 单纯 对远古存在的神 用谦卑的身份 默廊灯风纯 不将就人想 你已懂字事 泪水的距丽 你在水\n",
      "\n",
      "Epoch 200. Perplexity 2.165399\n",
      " -  分开我用不子错 我要着你的碎堡人 就想我 你知人 一天两 说你 经记忿婪热对恒 只对到力忠诚 干我带 半兽人 的灵魂 翻滚 停起忿存在的神 用谦卑的身份 走廊灯风纯 我摇要我疲微没到 你想就这样牵着 他身是\n",
      " -  不分开 已经 失去意义 戒指在哭泣 静静躺在抽屉 它所拥有的只剩下天都 等经的有音不住 沉 就儿风你怎么ㄟ！运一天 没有个 我给无将发了半 就一块龙梦 登底泰 以你的战役牲 周火几老著那 腿什么(客) 呼打我\n",
      " -  战争中部队的流瓣  为什么我爸始 那么凶手龙再已 放越黑　梦违背 有谁难安味 我的世界将被摧毁 也许颓废愿是另一种美 他许颓废也是另一种美 爷爷~~也翼 所事累 剩人你 的灵魂 翻滚 停止忿存 回无兽尽的战争 让\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn=rnn, is_random_iter=True, epochs=200, num_steps=35,\n",
    "                      hidden_dim=hidden_dim, learning_rate=0.2,\n",
    "                      clipping_theta=5, batch_size=32, pred_period=20,\n",
    "                      pred_len=100, seqs=seqs, get_params=get_params,\n",
    "                      get_inputs=get_inputs, ctx=ctx,\n",
    "                      corpus_indices=corpus_indices, idx_to_char=idx_to_char,\n",
    "                      char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再采用相邻批量采样实验循环神经网络谱写歌词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Perplexity 216.204097\n",
      " -  分开 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你\n",
      " -  不分开 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你\n",
      " -  战争中部队 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你\n",
      "\n",
      "Epoch 40. Perplexity 68.837397\n",
      " -  分开 你不要这样你 你知着你不是你 想不是我的爱你 让我们着你的天 我想想你的爱你 不会你的爱 有一种味 不要的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女\n",
      " -  不分开你想要 你不到你有你的你的天 放一种味 我想会这 你想了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你\n",
      " -  战争中部队 你不你再不能开 不要的没我想狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏\n",
      "\n",
      "Epoch 60. Perplexity 22.154746\n",
      " -  分开 说你的没太 我的世界 我很着你已 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想\n",
      " -  不分开你走 你不能你爱 我不能再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不\n",
      " -  战争中部队 我说你你已 我不能再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再\n",
      "\n",
      "Epoch 80. Perplexity 9.510521\n",
      " -  分开 这怪是这样的 唱什么我的是你 让我们 半兽人 的灵魂 单纯 对止忿存 永无止尽的战争 让我们 半兽人 的灵魂 单纯 对止忿存 永无止尽的战争 让我们 半兽人 的灵魂 单纯 对止忿存 永无止尽的战争 让\n",
      " -  不分开你走错样) 为是 我想再久了鱼 想是 你想再ㄟ了鱼 龙是 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想\n",
      " -  战争中部队要 你过去着你弃着 你身想慢你离开 难不是我不要你 是因些你 爱你已过去 还没有你 全 一子 你想再ㄟ气质 龙抹 你迷人ㄟ气质 龙抹 你想离久ㄟ鱼 放抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水\n",
      "\n",
      "Epoch 100. Perplexity 5.283618\n",
      " -  分开 这水的身匙我 一起的最现人 我怎么看不了我不想 你那像全你龙找 一身跳龙 把山我 轻水神 剩一 有杰忿存 永无止尽的战争 让我们 半兽人 的灵魂 单纯 停远古存在永恒 是谁为力忠 你的世界将被摧毁 也\n",
      " -  不分开你 这样爱我放腔水 可只会再手离开 这不是我连分开都迁就着你 我真的没有天份 安静的没这么快 我会学着放弃你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱\n",
      " -  战争中部队 (没有你 你不是过去 怀色蜡壶怎么 景下入秋 漫天黄沙凉过 塞北的客栈人多 牧草有没有 我马儿有些瘦 我就阅老国  什么我不想 你太着 是你不著后我有你太多 这日在然马 我不能 想情了望停才 你在人 \n",
      "\n",
      "Epoch 120. Perplexity 3.581256\n",
      " -  分开 这么的身旧我 不知的叹旧有 过去过去 像使边双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼\n",
      " -  不分开你走错样) 这愿 我想再再一直 龙的话望 在 变人 你想飞ㄟ了质 篮是 我想离水ㄟ鱼 放抹记 剩最后一口气 放抹记 剩抹记 不去 有止忿 x着 li xiu l炭 li xi xi xi xa xi x\n",
      " -  战争中部队一定的天度 我知道一只蜂你没要于我的那画面 经过苏美女神身边 我以不能 还颗心悬在静空 我只能够让分看著 这些看听了还让她知道 我是远会不见 你就着我 说你有 分数怎么停留 一直在停留 谁让它停你的 为\n",
      "\n",
      "Epoch 140. Perplexity 2.896909\n",
      " -  分开 这样的身匙我 一直的我现 这道情依的那一道 想记~~~~~~~~~ 爷 简！简！单！单！ 我想不能去没许 你身你说过我开微没来得  却吹的世的天找 我才你在你的依 一小是人和 我的世界将被摧毁 也许颓\n",
      " -  不分开你走错一定 没在你那经 所以你弃权走撑戏 ㄒー ㄌㄚ ㄙㄡ ㄈㄚ ㄇー 七我讲 风景这呢水 火车叨位去 七我讲 风景这呢水 火车叨位去 七纸它写歌里唱戏 ㄒー ㄌㄚ ㄙㄡ ㄈㄚ ㄇー 七我讲依歌对日我 \n",
      " -  战争中部队家就的你到 你来 想经表懦夫每 龙时 我想离ㄟㄟ鱼 龙抹记 剩抹后一口气 放抹记 剩最后一口气 放抹记 剩抹记 我想就这样牵着你的手不放开 爱可不可以简简单单没有伤害 你 靠着我的肩膀 你 在我胸口睡著\n",
      "\n",
      "Epoch 160. Perplexity 2.448149\n",
      " -  分开 这么的身里我 教着的最旧  是一起ㄟ祷  过i我很是有你说 别可太这样打  就连风吹都要干扰  可是你不想 一直走在黑暗地下道  想吹风想自由想要一起手牵手 去看海绕世界流浪 我害怕你心碎没人帮你擦眼\n",
      " -  不分开怎走 把手不慢在你我 难开球 别给我抬起头 有话去 医到箱锈的画言 说真的风旧出 教静光 看变 这一半 火炉烫 一步 是风人 x炉 li xo人 l炭 xi sou la xi xi xi xi xiu\n",
      " -  战争中部队纵 因要眼这来着就会出面 印象中的爱情好像顶不可见 我给你的爱写在西元前 深埋在美索不达米亚平原 几楔形世字后出土发现 那已风化千年的誓言 一切又重演 听司 神殿 征战 弓箭 是谁的从前 喜欢在人潮中你\n",
      "\n",
      "Epoch 180. Perplexity 2.302149\n",
      " -  分开 这么不在生打 后字都觉 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双\n",
      " -  不分开你走 你说你说爱子我 不开球 快给我抬起头 有话去对医药箱说 别怪我 别怪我 说你怎么面对我 甩开球我满腔的怒火 我想揍你已经很久 别想躲 说你眼睛看着我 别发抖 快给我抬起头 有话去对医药箱说 别怪我\n",
      " -  战争中部队 我爱你你可见 你不着 你爱不 一壶走步 再来一碗热粥 配上几斤的牛肉 我说店小二 三两银够不够 景色入秋 漫天黄沙凉过 塞北的客栈人多 牧草有没有 我马儿有些瘦 天涯尽头 我们上前生奏 我该好好生活 \n",
      "\n",
      "Epoch 200. Perplexity 2.066956\n",
      " -  分开 这么我的爱球 快沸腾 放异么都过去 我在等 灵魂序曲完成 带领族人里下祈祷文 想是轻大路里 让我碰到的只剩模找道 (爱小说 其实我的谁窗好有 什要我多没难分 但根个人已经不 我也耍的有模 快烁检红你 \n",
      " -  不分开怎走 把手慢慢交才 我就着陪国 查什么字眼 有人在降奏 蜥底横著走 三里拽斗牛 别里在篮落 有爽就篮走 别里在角落 有些就要走 三里在斗牛 有些就篮球 有里在篮牛 三种就斗牛 有话在篮牛 有里就篮走 别\n",
      " -  战争中部队然就 我才到你来碎没人帮你擦眼泪 别离那是非只要我们感觉对 我害怕你心碎没人帮你擦眼泪 别离开身非拥有你我的世界才能完美 你们你想逃开 你没有舍不得 你说你也会难过我不相信 牵着你陪着我 也只是曾经 希\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn=rnn, is_random_iter=False, epochs=200, num_steps=35,\n",
    "                      hidden_dim=hidden_dim, learning_rate=0.2,\n",
    "                      clipping_theta=5, batch_size=32, pred_period=20,\n",
    "                      pred_len=100, seqs=seqs, get_params=get_params,\n",
    "                      get_inputs=get_inputs, ctx=ctx,\n",
    "                      corpus_indices=corpus_indices, idx_to_char=idx_to_char,\n",
    "                      char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到一开始学到简单的字符，然后简单的词，接着是复杂点的词，然后看上去似乎像个句子了。\n",
    "\n",
    "## 结论\n",
    "\n",
    "* 通过隐含状态，循环神经网络适合处理前后有依赖关系时序数据样本。\n",
    "* 对前后有依赖关系时序数据样本批量采样时，我们可以使用随机批量采样和相邻批量采样。\n",
    "* 循环神经网络较容易出现梯度衰减和爆炸。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 调调参数（例如数据集大小、序列长度、隐含状态长度和学习率），看看对运行时间、perplexity和预测的结果造成的影响。\n",
    "* 在随机批量采样中，如果在同一个epoch中只把隐含变量在该epoch开始的时候初始化会怎么样？\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/989)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
