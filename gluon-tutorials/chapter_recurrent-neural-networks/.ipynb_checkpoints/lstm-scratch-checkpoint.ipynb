{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 长短期记忆（LSTM）--- 从0开始\n",
    "\n",
    "[上一节](bptt.md)中，我们介绍了循环神经网络中的梯度计算方法。我们发现，循环神经网络的隐含层变量梯度可能会出现衰减或爆炸。虽然[梯度裁剪](rnn-scratch.md)可以应对梯度爆炸，但无法解决梯度衰减的问题。因此，给定一个时间序列，例如文本序列，循环神经网络在实际中其实较难捕捉两个时刻距离较大的文本元素（字或词）之间的依赖关系。\n",
    "\n",
    "为了更好地捕捉时序数据中间隔较大的依赖关系，我们介绍了一种常用的门控循环神经网络，叫做[门控循环单元](gru-scratch.md)。本节将介绍另一种常用的门控循环神经网络，长短期记忆（long short-term memory，简称LSTM）。它由Hochreiter和Schmidhuber在1997年被提出。事实上，它比门控循环单元的结构稍微更复杂一点。\n",
    "\n",
    "\n",
    "## 长短期记忆\n",
    "\n",
    "我们先介绍长短期记忆的构造。长短期记忆的隐含状态包括隐含层变量$\\mathbf{H}$和细胞$\\mathbf{C}$（也称记忆细胞）。它们形状相同。\n",
    "\n",
    "\n",
    "### 输入门、遗忘门和输出门\n",
    "\n",
    "\n",
    "假定隐含状态长度为$h$，给定时刻$t$的一个样本数为$n$特征向量维度为$x$的批量数据$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$和上一时刻隐含状态$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$，输入门（input gate）$\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$、遗忘门（forget gate）$\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$和输出门（output gate）$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$的定义如下：\n",
    "\n",
    "$$\\mathbf{I}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i)$$\n",
    "\n",
    "$$\\mathbf{F}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f)$$\n",
    "\n",
    "$$\\mathbf{O}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o)$$\n",
    "\n",
    "其中的$\\mathbf{W}_{xi}, \\mathbf{W}_{xf}, \\mathbf{W}_{xo} \\in \\mathbb{R}^{x \\times h}$和$\\mathbf{W}_{hi}, \\mathbf{W}_{hf}, \\mathbf{W}_{ho} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_i, \\mathbf{b}_f, \\mathbf{b}_o \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。函数$\\sigma$自变量中的三项相加使用了[广播](../chapter_crashcourse/ndarray.md)。\n",
    "\n",
    "和[门控循环单元](gru-scratch.md)中的重置门和更新门一样，这里的输入门、遗忘门和输出门中每个元素的值域都是$[0, 1]$。\n",
    "\n",
    "\n",
    "### 候选细胞\n",
    "\n",
    "和[门控循环单元](gru-scratch.md)中的候选隐含状态一样，长短期记忆中的候选细胞$\\tilde{\\mathbf{C}}_t \\in \\mathbb{R}^{n \\times h}$也使用了值域在$[-1, 1]$的双曲正切函数tanh做激活函数：\n",
    "\n",
    "$$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c)$$\n",
    "\n",
    "其中的$\\mathbf{W}_{xc} \\in \\mathbb{R}^{x \\times h}$和$\\mathbf{W}_{hc} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_c \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。\n",
    "\n",
    "\n",
    "### 细胞\n",
    "\n",
    "我们可以通过元素值域在$[0, 1]$的输入门、遗忘门和输出门来控制隐含状态中信息的流动：这通常可以应用按元素乘法符$\\odot$。当前时刻细胞$\\mathbf{C}_t \\in \\mathbb{R}^{n \\times h}$的计算组合了上一时刻细胞和当前时刻候选细胞的信息，并通过遗忘门和输入门来控制信息的流动：\n",
    "\n",
    "$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$$\n",
    "\n",
    "需要注意的是，如果遗忘门一直近似1且输入门一直近似0，过去的细胞将一直通过时间保存并传递至当前时刻。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时序数据中间隔较大的依赖关系。\n",
    "\n",
    "\n",
    "### 隐含状态\n",
    "\n",
    "有了细胞以后，接下来我们还可以通过输出门来控制从细胞到隐含层变量$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$的信息的流动：\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\text{tanh}(\\mathbf{C}_t)$$\n",
    "\n",
    "需要注意的是，当输出门近似1，细胞信息将传递到隐含层变量；当输出门近似0，细胞信息只自己保留。\n",
    "\n",
    "\n",
    "\n",
    "输出层的设计可参照[循环神经网络](rnn-scratch.md)中的描述。\n",
    "\n",
    "\n",
    "## 实验\n",
    "\n",
    "\n",
    "为了实现并展示门控循环单元，我们依然使用周杰伦歌词数据集来训练模型作词。这里除长短期记忆以外的实现已在[循环神经网络](rnn-scratch.md)中介绍。\n",
    "\n",
    "\n",
    "### 数据处理\n",
    "\n",
    "我们先读取并对数据集做简单处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 1465\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip', 'r') as zin:\n",
    "    zin.extractall('../data/')\n",
    "\n",
    "with open('../data/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars = f.read()\n",
    "\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:20000]\n",
    "\n",
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "print('vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用onehot来将字符索引表示成向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(data):\n",
    "    return [nd.one_hot(X, vocab_size) for X in data.T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数\n",
    "\n",
    "以下部分对模型参数进行初始化。参数`hidden_dim`定义了隐含状态的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use gpu(0)\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "# 尝试使用GPU\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from mxnet import nd\n",
    "import utils\n",
    "ctx = utils.try_gpu()\n",
    "print('Will use', ctx)\n",
    "\n",
    "input_dim = vocab_size\n",
    "# 隐含状态长度\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "std = .01\n",
    "\n",
    "def get_params():\n",
    "    # 输入门参数\n",
    "    W_xi = nd.random_normal(scale=std, shape=(input_dim, hidden_dim), ctx=ctx)\n",
    "    W_hi = nd.random_normal(scale=std, shape=(hidden_dim, hidden_dim), ctx=ctx)\n",
    "    b_i = nd.zeros(hidden_dim, ctx=ctx)\n",
    "    \n",
    "    # 遗忘门参数\n",
    "    W_xf = nd.random_normal(scale=std, shape=(input_dim, hidden_dim), ctx=ctx)\n",
    "    W_hf = nd.random_normal(scale=std, shape=(hidden_dim, hidden_dim), ctx=ctx)\n",
    "    b_f = nd.zeros(hidden_dim, ctx=ctx)\n",
    "    \n",
    "    # 输出门参数\n",
    "    W_xo = nd.random_normal(scale=std, shape=(input_dim, hidden_dim), ctx=ctx)\n",
    "    W_ho = nd.random_normal(scale=std, shape=(hidden_dim, hidden_dim), ctx=ctx)\n",
    "    b_o = nd.zeros(hidden_dim, ctx=ctx)\n",
    "\n",
    "    # 候选细胞参数\n",
    "    W_xc = nd.random_normal(scale=std, shape=(input_dim, hidden_dim), ctx=ctx)\n",
    "    W_hc = nd.random_normal(scale=std, shape=(hidden_dim, hidden_dim), ctx=ctx)\n",
    "    b_c = nd.zeros(hidden_dim, ctx=ctx)\n",
    "\n",
    "    # 输出层\n",
    "    W_hy = nd.random_normal(scale=std, shape=(hidden_dim, output_dim), ctx=ctx)\n",
    "    b_y = nd.zeros(output_dim, ctx=ctx)\n",
    "\n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n",
    "              b_c, W_hy, b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "我们将前面的模型公式翻译成代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_rnn(inputs, state_h, state_c, *params):\n",
    "    # inputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵\n",
    "    # H: 尺寸为 batch_size * hidden_dim 矩阵\n",
    "    # outputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "     W_hy, b_y] = params\n",
    "\n",
    "    H = state_h\n",
    "    C = state_c\n",
    "    outputs = []\n",
    "    for X in inputs:        \n",
    "        I = nd.sigmoid(nd.dot(X, W_xi) + nd.dot(H, W_hi) + b_i)\n",
    "        F = nd.sigmoid(nd.dot(X, W_xf) + nd.dot(H, W_hf) + b_f)\n",
    "        O = nd.sigmoid(nd.dot(X, W_xo) + nd.dot(H, W_ho) + b_o)\n",
    "        C_tilda = nd.tanh(nd.dot(X, W_xc) + nd.dot(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * nd.tanh(C)\n",
    "        Y = nd.dot(H, W_hy) + b_y\n",
    "        outputs.append(Y)\n",
    "    return (outputs, H, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "下面我们开始训练模型。我们假定谱写歌词的前缀分别为“分开”、“不分开”和“战争中部队”。这里采用的是相邻批量采样实验门控循环单元谱写歌词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Training perplexity 320.642718\n",
      " -  分开 我我的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 \n",
      " -  不分开 我我的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 \n",
      " -  战争中部队 我我的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 \n",
      "\n",
      "Epoch 40. Training perplexity 180.042395\n",
      " -  分开 我想你你想你的可爱女人 我想你你你你的可爱女 我想你你你你你的可爱女人 我想你你你你的可爱女 我想你你你你你的可爱女人 我想你你你你的可爱女 我想你你你你你的可爱女人 我想你你你你的可爱女 我想你你你\n",
      " -  不分开 我想你你你想你的可爱女人 我想你你你你的可爱女 我想你你你你你的可爱女人 我想你你你你的可爱女 我想你你你你你的可爱女人 我想你你你你的可爱女 我想你你你你你的可爱女人 我想你你你你的可爱女 我想你你\n",
      " -  战争中部队 我想你你你想你的可爱女人 我想你你你你的可爱女 我想你你你你你的可爱女人 我想你你你你的可爱女 我想你你你你你的可爱女人 我想你你你你的可爱女 我想你你你你你的可爱女人 我想你你你你的可爱女 我想你你\n",
      "\n",
      "Epoch 60. Training perplexity 74.286568\n",
      " -  分开 我不要你的爱 我不要 你不我 想不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我\n",
      " -  不分开 我不要你的爱 我不要 你不不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我\n",
      " -  战争中部队 我想不这不开 你不要 你不我 想不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我\n",
      "\n",
      "Epoch 80. Training perplexity 27.742082\n",
      " -  分开 一直两人在叫你 我是你 一直人 的灵魂 翻滚 停止 xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi\n",
      " -  不分开 我不不要你开开 为什么我爱你你 爱因为我爱你 是你着我爱你 你爱着你爱你 是你的让你面的你爱爱你 爱你在我爱你 可爱着你 你爱你的爱爱的有有卷 温柔的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏\n",
      " -  战争中部队 他在泡的茶 听一种味道叫做家 陆羽泡的茶 像一种味道叫做家 陆羽泡的茶 像一种味道叫做家 陆羽泡的茶 像一种味道叫做家 陆羽泡的茶 像一种味道叫做家 陆羽泡的茶 像一种味道叫做家 陆羽泡的茶 像一种味\n",
      "\n",
      "Epoch 100. Training perplexity 11.158366\n",
      " -  分开 一直我的太画 我右 你想很了我 这去 你想很久了吧? 我想想你远远著你 别那你 你给我 别不怎么对我怎么 你想要你 你不我 别不球 我该 我对 我不 我不 我不 我不 我不 我不要 爱情走的太快就像龙\n",
      " -  不分开 我不不要 你不了这节 我该该这样活 后知不觉 你已开离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个 我该好好生活 后知不觉 你已了离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个 我该好好生\n",
      " -  战争中部队 在真的手的防 的一天在我的那里  想想你的想 一定在美起 泪静的脸 有一种美义 一色的旧栈人 你在你 半兽人 的灵魂 单纯 停止止恨 永忆止尽的战争 让我们 半兽人 的灵魂 单纯 停止忿存在永恒 火ー\n",
      "\n",
      "Epoch 120. Training perplexity 5.339658\n",
      " -  分开 是我连的话面 没通 你又很久了吧? 我想想 你给我只想要你 陪我去 你给我 别想球 我怎么这了你 想开去我满腔的怒火 我想揍你已经很久 别想躲 说你眼睛看着我 别发抖 快给我抬起头 有话去对医药 说说\n",
      " -  不分开 我只不要你离开 为什么我连经 是你都 我爱你爱了你 陪我去么快堡 我说你 你爱我 想 简！简！单！！！ 爱~~~~~~~~~~ 想 简！简！单单单！ 爱~~~ 你~会听到 我不要 你不再口我 不这样样\n",
      " -  战争中部队就抽 爷爷的手 在一种味道叫做家 他满着一匹 口感味觉准不差 陆羽泡的茶 像说名和利都不拿 爷爷泡的茶 有一种味道叫做家 他满泡的茶 有一种味道叫做家 他满泡的茶 有一种味道叫做家 他满泡的茶 有一种味\n",
      "\n",
      "Epoch 140. Training perplexity 3.149070\n",
      " -  分开 你那那我已分堡 没有你 一直你我说想 说你啊你 我不是这样活 你知啊觉生活 不知了悄默默开开 陷入了危 荣给我变不得 一话走着我满多 快多 我想你睡了吧?  太太你给汉堡  说穿了其实我 不开不着我太\n",
      " -  不分开 我就了陪你弃开 为为为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我\n",
      " -  战争中部队就就你的脸 古边你 在着着人开始 回管来 说不开 我想了这样对着你的黑空面走  不再的旧里天看了看 太不是我的你我说知你你深那那 我 在你的那爸在 你会想是你睡著著我 在小的黑壕你了了了一起 她一个中不\n",
      "\n",
      "Epoch 160. Training perplexity 2.084224\n",
      " -  分开 是你那在妈打 从色看看头 是一在人落 你爱于 庄边边下在在一直 痛到一片 铁铁的钥匙 还在茶里 装蟑螂 木炭 从一直 x炭 xi xi xa xi xi xi xi xi xi xi xi xi xi\n",
      " -  不分开 我手不能分 我怎 你爱 我想 简！简！单单单！ 爱~ 爱~~~快沉默 周不该到 我不了些犹活 我知 却又再考倒我 说你 你想很久了吧? 我的太面给给默默默默 别过说你多子我妈止止来的手 放容说风我开始\n",
      " -  战争中部队纵 在你在你的风 一一天名叫叫岛盒 他在泡的茶 有一种味道叫做家 他羽泡的茶 口幅泼墨的山水画 一朝黄黄旧旧的日 现间在旁里的瞒晨 回何侵底都么我进的天堂的小度 在古道被废弃的白蛦丘 站着一只饿昏的老斑\n",
      "\n",
      "Epoch 180. Training perplexity 1.619316\n",
      " -  分开 这为我的话怪  说下 说你开了我说 说说抖 是不是一起球 你在为你面药箱  上上我的爱笑 你说好 你爱我开难难你 你怎么这样打我 甩开球我满腔的怒火 就想揍个已已几止止 我说 你不会不想 你 在我面的\n",
      " -  不分开 我要要慢慢 我留 陪笑 周杰 周杰伦了周杰 周变伦 它隐箱一切南 我想 你已很倒倒? 这上的你的忧  我想要你已汉命 没想要没太你 我也下 让笑常我妈你 说要啊 你不是你说球 说你怎么面对我 甩开球我\n",
      " -  战争中部队就旁你的脸 好边的最你在风地的凯是琳主年 专以 消AB血型的公老鼠 恍恍惚惚 是谁的脚步 银制茶壶 装蟑螂蜘蛛 辛辛苦苦 全家怕日出 白色蜡烛 温暖了空屋 白色蜡烛 温暖了空屋 白色蜡烛 温暖了空屋 白\n",
      "\n",
      "Epoch 200. Training perplexity 1.382699\n",
      " -  分开 这直在扑满妈破家 (小小愿望就快实现了 他在笑) 他真的真的想知道 那首来自东欧的民谣 和弦到底什么调 寻马碲铁还要 敲多少吉他才能买得 他真的没知知 安静的旧旧气 我想你已表表的非常明白 我懂你让知\n",
      " -  不分开 我手的陪你走 雨望不觉 我跟了你难开 我知不觉 我不了这节我 你知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活 不知不觉 你已经离开我 不知不觉 我跟了这节奏 后知\n",
      " -  战争中部队堂旁你会手 好边间 一颗好颗 江一起很六页七盒 ㄒ小 ㄌㄚ飞建的冰实 钟蹄等 半步克 的让神 单纯 停止忿恨 永无止尽的战争 让我们 半兽人 的灵魂 单纯 对远古存在永恒 只对暴力忠诚 让我们 半兽人 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq1 = '分开'\n",
    "seq2 = '不分开'\n",
    "seq3 = '战争中部队'\n",
    "seqs = [seq1, seq2, seq3]\n",
    "\n",
    "utils.train_and_predict_rnn(rnn=lstm_rnn, is_random_iter=False, epochs=200,\n",
    "                            num_steps=35, hidden_dim=hidden_dim, \n",
    "                            learning_rate=0.2, clipping_norm=5,\n",
    "                            batch_size=32, pred_period=20, pred_len=100,\n",
    "                            seqs=seqs, get_params=get_params,\n",
    "                            get_inputs=get_inputs, ctx=ctx,\n",
    "                            corpus_indices=corpus_indices,\n",
    "                            idx_to_char=idx_to_char, char_to_idx=char_to_idx,\n",
    "                            is_lstm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到一开始学到简单的字符，然后简单的词，接着是复杂点的词，然后看上去似乎像个句子了。\n",
    "\n",
    "## 结论\n",
    "\n",
    "* 长短期记忆的提出是为了更好地捕捉时序数据中间隔较大的依赖关系。\n",
    "* 长短期记忆的结构比门控循环单元的结构较复杂。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 调调参数（例如数据集大小、序列长度、隐含状态长度和学习率），看看对运行时间、perplexity和预测的结果造成的影响。\n",
    "* 在相同条件下，比较长短期记忆和门控循环单元以及循环神经网络的运行效率。\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/4042)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
