{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络 --- 从0开始\n",
    "\n",
    "前面的教程里我们使用的网络都属于**前馈神经网络**。之所以叫前馈，是因为整个网络是一条链（回想下`gluon.nn.Sequential`），每一层的结果都是反馈给下一层。这一节我们介绍**循环神经网络**，这里每一层不仅输出给下一层，同时还输出一个**隐含状态**，给当前层在处理下一个样本时使用。下图展示这两种网络的区别。\n",
    "\n",
    "![](../img/rnn_1.png)\n",
    "\n",
    "循环神经网络的这种结构使得它适合处理前后有依赖关系数据样本。我们拿语言模型举个例子来解释这个是怎么工作的。语言模型的任务是给定句子的前*t*个字符，然后预测第*t+1*个字符。假设我们的句子是“你好世界”，使用前馈神经网络来预测的一个做法是，在时间1输入“你”，预测”好“，时间2向同一个网络输入“好”预测“世”。下图左边展示了这个过程。\n",
    "\n",
    "![](../img/rnn_2.png)\n",
    "\n",
    "注意到一个问题是，当我们预测“世”的时候只给了“好”这个输入，而完全忽略了“你”。直觉上“你”这个词应该对这次的预测比较重要。虽然这个问题通常可以通过**n-gram**来缓解，就是说预测第*t+1*个字符的时候，我们输入前*n*个字符。如果*n=1*，那就是我们这里用的。我们可以增大*n*来使得输入含有更多信息。但我们不能任意增大*n*，因为这样通常带来模型复杂度的增加从而导致需要大量数据和计算来训练模型。\n",
    "\n",
    "循环神经网络使用一个隐含状态来记录前面看到的数据来帮助当前预测。上图右边展示了这个过程。在预测“好”的时候，我们输出一个隐含状态。我们用这个状态和新的输入“好”来一起预测“世”，然后同时输出一个更新过的隐含状态。我们希望前面的信息能够保存在这个隐含状态里，从而提升预测效果。\n",
    "\n",
    "## 循环神经网络\n",
    "\n",
    "在对输入输出数据有了解后，我们来正式介绍循环神经网络。\n",
    "\n",
    "首先回忆一下单隐含层的前馈神经网络的定义，例如[多层感知机](../chapter_supervised-learning/mlp-scratch.md)。假设隐含层的激活函数是$\\phi$，对于一个样本数为$n$特征向量维度为$x$的批量数据$\\mathbf{X} \\in \\mathbb{R}^{n \\times x}$（$\\mathbf{X}$是一个$n$行$x$列的实数矩阵）来说，那么这个隐含层的输出就是\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h)$$\n",
    "\n",
    "假定隐含层长度为$h$，其中的$\\mathbf{W}_{xh} \\in \\mathbb{R}^{x \\times h}$是权重参数。偏移参数 $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$在与前一项$\\mathbf{X} \\mathbf{W}_{xh} \\in \\mathbb{R}^{n \\times h}$ 相加时使用了[广播](../chapter_crashcourse/ndarray.md)。这个隐含层的输出的尺寸为$\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$。\n",
    "\n",
    "把隐含层的输出$\\mathbf{H}$作为输出层的输入，最终的输出\n",
    "\n",
    "$$\\hat{\\mathbf{Y}} = \\text{softmax}(\\mathbf{H} \\mathbf{W}_{hy} + \\mathbf{b}_y)$$\n",
    "\n",
    "假定每个样本对应的输出向量维度为$y$，其中 $\\hat{\\mathbf{Y}} \\in \\mathbb{R}^{n \\times y}, \\mathbf{W}_{hy} \\in \\mathbb{R}^{h \\times y}, \\mathbf{b}_y \\in \\mathbb{R}^{1 \\times y}$且两项相加使用了[广播](../chapter_crashcourse/ndarray.md)。\n",
    "\n",
    "\n",
    "将上面网络改成循环神经网络，我们首先对输入输出加上时间戳$t$。假设$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$是序列中的第$t$个批量输入（样本数为$n$，每个样本的特征向量维度为$x$），对应的隐含层输出是隐含状态$\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$（隐含层长度为$h$），而对应的最终输出是$\\hat{\\mathbf{Y}}_t \\in \\mathbb{R}^{n \\times y}$（每个样本对应的输出向量维度为$y$）。在计算隐含层的输出的时候，循环神经网络只需要在前馈神经网络基础上加上跟前一时间$t-1$输入隐含层$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$的加权和。为此，我们引入一个新的可学习的权重$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$：\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h)$$\n",
    "\n",
    "输出的计算跟前面一致：\n",
    "\n",
    "$$\\hat{\\mathbf{Y}}_t = \\text{softmax}(\\mathbf{H}_t \\mathbf{W}_{hy}  + \\mathbf{b}_y)$$\n",
    "\n",
    "一开始我们提到过，隐含状态可以认为是这个网络的记忆。该网络中，时刻$t$的隐含状态就是该时刻的隐含层变量$\\mathbf{H}_t$。它存储前面时间里面的信息。我们的输出是只基于这个状态。最开始的隐含状态里的元素通常会被初始化为0。\n",
    "\n",
    "\n",
    "## 周杰伦歌词数据集\n",
    "\n",
    "\n",
    "为了实现并展示循环神经网络，我们使用周杰伦歌词数据集来训练模型作词。该数据集里包含了著名创作型歌手周杰伦从第一张专辑《Jay》到第十张专辑《跨时代》所有歌曲的歌词。\n",
    "\n",
    "![](../img/jay.jpg)\n",
    "\n",
    "\n",
    "下面我们读取这个数据并看看前面49个字符（char）是什么样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip', 'r') as zin:\n",
    "    zin.extractall('../data/')\n",
    "\n",
    "with open('../data/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars = f.read()\n",
    "print(corpus_chars[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看一下数据集里的字符数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print len(corpus_chars)\n",
    "type(corpus_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们稍微处理下数据集。为了打印方便，我们把换行符替换成空格，然后截去后面一段使得接下来的训练会快一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字符的数值表示\n",
    "\n",
    "先把数据里面所有不同的字符拿出来做成一个字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vocab size:', 141)\n"
     ]
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "print('vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后可以把每个字符转成从0开始的索引(index)来方便之后的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: \n",
      "想要有直升机 想要和你飞到宇\n",
      "\n",
      "indices: \n",
      "[136, 0, 15, 66, 114, 71, 136, 43, 73, 29, 6, 52, 98, 74, 1, 136, 43, 123, 8, 136, 0, 15, 66, 114, 71, 98, 108, 39, 64, 89, 44, 100, 9, 111, 98, 38, 50, 98, 118, 1]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "\n",
    "sample = corpus_indices[:40]\n",
    "\n",
    "print 'chars: \\n', ''.join([idx_to_char[idx] for idx in sample])\n",
    "print '\\nindices: \\n', sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时序数据的批量采样\n",
    "\n",
    "同之前一样我们需要每次随机读取一些（`batch_size`个）样本和其对用的标号。这里的样本跟前面有点不一样，这里一个样本通常包含一系列连续的字符（前馈神经网络里可能每个字符作为一个样本）。\n",
    "\n",
    "如果我们把序列长度（`num_steps`）设成5，那么一个可能的样本是“想要有直升”。其对应的标号仍然是长为5的序列，每个字符是对应的样本里字符的后面那个。例如前面样本的标号就是“要有直升机”。\n",
    "\n",
    "\n",
    "### 随机批量采样\n",
    "\n",
    "下面代码每次从数据里随机采样一个批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from mxnet import nd\n",
    "\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    # 减一是因为label的索引是相应data的索引加一\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    # 随机化样本\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    # 返回num_steps个数据\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos: pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "        data = nd.array(\n",
    "            [_data(j * num_steps) for j in batch_indices], ctx=ctx)\n",
    "        label = nd.array(\n",
    "            [_data(j * num_steps + 1) for j in batch_indices], ctx=ctx)\n",
    "        yield data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了便于理解时序数据上的随机批量采样，让我们输入一个从0到29的人工序列，看下读出来长什么样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  \n",
      "[[ 15.  16.  17.]\n",
      " [  3.   4.   5.]\n",
      " [ 24.  25.  26.]\n",
      " [ 12.  13.  14.]]\n",
      "<NDArray 4x3 @cpu(0)> \n",
      "label: \n",
      "[[ 16.  17.  18.]\n",
      " [  4.   5.   6.]\n",
      " [ 25.  26.  27.]\n",
      " [ 13.  14.  15.]]\n",
      "<NDArray 4x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[  0.   1.   2.]\n",
      " [  6.   7.   8.]\n",
      " [  9.  10.  11.]\n",
      " [ 21.  22.  23.]]\n",
      "<NDArray 4x3 @cpu(0)> \n",
      "label: \n",
      "[[  1.   2.   3.]\n",
      " [  7.   8.   9.]\n",
      " [ 10.  11.  12.]\n",
      " [ 22.  23.  24.]]\n",
      "<NDArray 4x3 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "\n",
    "for data, label in data_iter_random(my_seq, batch_size=4, num_steps=3):\n",
    "    print 'data: ', data, '\\nlabel:', label, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于各个采样在原始序列上的位置是随机的时序长度为`num_steps`的连续数据点，相邻的两个随机批量在原始序列上的位置不一定相毗邻。因此，在训练模型时，读取每个随机时序批量前需要重新初始化隐含状态。\n",
    "\n",
    "\n",
    "### 相邻批量采样\n",
    "\n",
    "除了对原序列做随机批量采样之外，我们还可以使相邻的两个随机批量在原始序列上的位置相毗邻。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "\n",
    "    indices = corpus_indices[0: batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    # 减一是因为label的索引是相应data的索引加一\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        data = indices[:, i: i + num_steps]\n",
    "        label = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相同地，为了便于理解时序数据上的相邻批量采样，让我们输入一个从0到29的人工序列，看下读出来长什么样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  \n",
      "[[  0.   1.   2.]\n",
      " [ 15.  16.  17.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[  1.   2.   3.]\n",
      " [ 16.  17.  18.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[  3.   4.   5.]\n",
      " [ 18.  19.  20.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[  4.   5.   6.]\n",
      " [ 19.  20.  21.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[  6.   7.   8.]\n",
      " [ 21.  22.  23.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[  7.   8.   9.]\n",
      " [ 22.  23.  24.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[  9.  10.  11.]\n",
      " [ 24.  25.  26.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 10.  11.  12.]\n",
      " [ 25.  26.  27.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "\n",
    "for data, label in data_iter_consecutive(my_seq, batch_size=2, num_steps=3):\n",
    "    print 'data: ', data, '\\nlabel:', label, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于各个采样在原始序列上的位置是毗邻的时序长度为`num_steps`的连续数据点，因此，使用相邻批量采样训练模型时，读取每个时序批量前，我们需要将该批量最开始的隐含状态设为上个批量最终输出的隐含状态。在同一个epoch中，隐含状态只需要在该epoch开始的时候初始化。\n",
    "\n",
    "\n",
    "## One-hot向量\n",
    "\n",
    "注意到每个字符现在是用一个整数来表示，而输入进网络我们需要一个定长的向量。一个常用的办法是使用one-hot来将其表示成向量。也就是说，如果一个字符的整数值是$i$, 那么我们创建一个全0的长为`vocab_size`的向量，并将其第$i$位设成1。该向量就是对原字符的one-hot向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
       " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
       "<NDArray 2x88 @cpu(0)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.one_hot(nd.array([0, 2]), vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记得前面我们每次得到的数据是一个`batch_size * num_steps`的批量。下面这个函数将其转换成`num_steps`个可以输入进网络的`batch_size * vocab_size`的矩阵。对于一个长度为`num_steps`的序列，每个批量输入$\\mathbf{X} \\in \\mathbb{R}^{n \\times x}$，其中$n=$ `batch_size`，而$x=$`vocab_size`（onehot编码向量维度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input length: ', 3)\n",
      "('input[0] shape: ', (2L, 88L))\n"
     ]
    }
   ],
   "source": [
    "def get_inputs(data):\n",
    "    return [nd.one_hot(X, vocab_size) for X in data.T]\n",
    "\n",
    "inputs = get_inputs(data)\n",
    "\n",
    "print('input length: ', len(inputs))\n",
    "print('input[0] shape: ', inputs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "对于序列中任意一个时间戳，一个字符的输入是维度为`vocab_size`的one-hot向量，对应输出是预测下一个时间戳为词典中任意字符的概率，因而该输出是维度为`vocab_size`的向量。\n",
    "\n",
    "当序列中某一个时间戳的输入为一个样本数为`batch_size`（对应模型定义中的$n$）的批量，每个时间戳上的输入和输出皆为尺寸`batch_size * vocab_size`（对应模型定义中的$n \\times x$）的矩阵。假设每个样本对应的隐含状态的长度为`hidden_dim`（对应模型定义中隐含层长度$h$），根据矩阵乘法定义，我们可以推断出模型隐含层和输出层中各个参数的尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Will use', cpu(0))\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "# 尝试使用GPU\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "ctx = utils.try_gpu()\n",
    "print('Will use', ctx)\n",
    "\n",
    "input_dim = vocab_size\n",
    "# 隐含状态长度\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "std = .01\n",
    "\n",
    "def get_params():\n",
    "    # 隐含层\n",
    "    W_xh = nd.random_normal(scale=std, shape=(input_dim, hidden_dim), ctx=ctx)\n",
    "    W_hh = nd.random_normal(scale=std, shape=(hidden_dim, hidden_dim), ctx=ctx)\n",
    "    b_h = nd.zeros(hidden_dim, ctx=ctx)\n",
    "\n",
    "    # 输出层\n",
    "    W_hy = nd.random_normal(scale=std, shape=(hidden_dim, output_dim), ctx=ctx)\n",
    "    b_y = nd.zeros(output_dim, ctx=ctx)\n",
    "\n",
    "    params = [W_xh, W_hh, b_h, W_hy, b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "当序列中某一个时间戳的输入为一个样本数为`batch_size`的批量，而整个序列长度为`num_steps`时，以下`rnn`函数的`inputs`和`outputs`皆为`num_steps` 个尺寸为`batch_size * vocab_size`的矩阵，隐含变量$\\mathbf{H}$是一个尺寸为`batch_size * hidden_dim`的矩阵。该隐含变量$\\mathbf{H}$也是循环神经网络的隐含状态`state`。\n",
    "\n",
    "我们将前面的模型公式翻译成代码。这里的激活函数使用了按元素操作的双曲正切函数\n",
    "\n",
    "$$\\text{tanh}(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}$$\n",
    "\n",
    "需要注意的是，双曲正切函数的值域是$[-1, 1]$。如果自变量均匀分布在整个实域，该激活函数输出的均值为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn(inputs, state, *params):\n",
    "    # inputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵。\n",
    "    # H: 尺寸为 batch_size * hidden_dim 矩阵。\n",
    "    # outputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵。\n",
    "    H = state\n",
    "    W_xh, W_hh, b_h, W_hy, b_y = params\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)\n",
    "        Y = nd.dot(H, W_hy) + b_y\n",
    "        outputs.append(Y)\n",
    "    return (outputs, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做个简单的测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('output length: ', 3)\n",
      "('output[0] shape: ', (2L, 88L))\n",
      "('state shape: ', (2L, 256L))\n"
     ]
    }
   ],
   "source": [
    "state = nd.zeros(shape=(data.shape[0], hidden_dim), ctx=ctx)\n",
    "\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(get_inputs(data.as_in_context(ctx)), state, *params)\n",
    "\n",
    "print('output length: ',len(outputs))\n",
    "print('output[0] shape: ', outputs[0].shape)\n",
    "print('state shape: ', state_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测序列\n",
    "\n",
    "在做预测时我们只需要给定时间0的输入和起始隐含变量。然后我们每次将上一个时间的输出作为下一个时间的输入。\n",
    "\n",
    "![](../img/rnn_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_rnn(rnn, prefix, num_chars, params, hidden_dim, ctx, idx_to_char,\n",
    "                char_to_idx, get_inputs, is_lstm=False):\n",
    "    # 预测以 prefix 开始的接下来的 num_chars 个字符。\n",
    "    prefix = prefix.lower()\n",
    "    state_h = nd.zeros(shape=(1, hidden_dim), ctx=ctx)\n",
    "    if is_lstm:\n",
    "        # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "        state_c = nd.zeros(shape=(1, hidden_dim), ctx=ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for i in range(num_chars + len(prefix)):\n",
    "        X = nd.array([output[-1]], ctx=ctx)\n",
    "        # 在序列中循环迭代隐含变量。\n",
    "        if is_lstm:\n",
    "            # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "            Y, state_h, state_c = rnn(get_inputs(X), state_h, state_c, *params)\n",
    "        else:\n",
    "            Y, state_h = rnn(get_inputs(X), state_h, *params)\n",
    "        if i < len(prefix)-1:\n",
    "            next_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            next_input = int(Y[0].argmax(axis=1).asscalar())\n",
    "        output.append(next_input)\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度剪裁\n",
    "\n",
    "我们在[正向传播和反向传播](../chapter_supervised-learning/backprop.md)中提到，\n",
    "训练神经网络往往需要依赖梯度计算的优化算法，例如我们之前介绍的[随机梯度下降](../chapter_supervised-learning/linear-regression-scratch.md)。\n",
    "而在循环神经网络的训练中，当每个时序训练数据样本的时序长度`num_steps`较大或者时刻$t$较小，目标函数有关$t$时刻的隐含层变量梯度较容易出现衰减（valishing）或爆炸（explosion）。我们会在[下一节](bptt.md)详细介绍出现该现象的原因。\n",
    "\n",
    "为了应对梯度爆炸，一个常用的做法是如果梯度特别大，那么就投影到一个比较小的尺度上。假设我们把所有梯度接成一个向量 $\\boldsymbol{g}$，假设剪裁的阈值是$\\theta$，那么我们这样剪裁使得$\\|\\boldsymbol{g}\\|$不会超过$\\theta$：\n",
    "\n",
    "$$ \\boldsymbol{g} = \\min\\left(\\frac{\\theta}{\\|\\boldsymbol{g}\\|}, 1\\right)\\boldsymbol{g}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, ctx):\n",
    "    if theta is not None:\n",
    "        norm = nd.array([0.0], ctx)\n",
    "        for p in params:\n",
    "            norm += nd.sum(p.grad ** 2)\n",
    "        norm = nd.sqrt(norm).asscalar()\n",
    "        if norm > theta:\n",
    "            for p in params:\n",
    "                p.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "下面我们可以还是训练模型。跟前面前置网络的教程比，这里有以下几个不同。\n",
    "\n",
    "1. 通常我们使用困惑度（Perplexity）这个指标。\n",
    "2. 在更新前我们对梯度做剪裁。\n",
    "3. 在训练模型时，对时序数据采用不同批量采样方法将导致隐含变量初始化的不同。\n",
    "\n",
    "### 困惑度（Perplexity）\n",
    "\n",
    "回忆以下我们之前介绍的[交叉熵损失函数](../chapter_supervised-learning/softmax-regression-scratch.md)。在语言模型中，该损失函数即被预测字符的对数似然平均值的相反数：\n",
    "\n",
    "$$\\text{loss} = -\\frac{1}{N} \\sum_{i=1}^N \\log p_{\\text{target}_i}$$\n",
    "\n",
    "其中$N$是预测的字符总数，$p_{\\text{target}_i}$是在第$i$个预测中真实的下个字符被预测的概率。\n",
    "\n",
    "而这里的困惑度可以简单的认为就是对交叉熵做exp运算使得数值更好读。\n",
    "\n",
    "为了解释困惑度的意义，我们先考虑一个完美结果：模型总是把真实的下个字符的概率预测为1。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 1$。这种完美情况下，困惑度值为1。\n",
    "\n",
    "我们再考虑一个基线结果：给定不重复的字符集合$W$及其字符总数$|W|$，模型总是预测下个字符为集合$W$中任一字符的概率都相同。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 1/|W|$。这种基线情况下，困惑度值为$|W|$。\n",
    "\n",
    "最后，我们可以考虑一个最坏结果：模型总是把真实的下个字符的概率预测为0。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 0$。这种最坏情况下，困惑度值为正无穷。\n",
    "\n",
    "任何一个有效模型的困惑度值必须小于预测集中元素的数量。在本例中，困惑度必须小于字典中的字符数$|W|$。如果一个模型可以取得较低的困惑度的值（更靠近1），通常情况下，该模型预测更加准确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from math import exp\n",
    "           \n",
    "def train_and_predict_rnn(rnn, is_random_iter, epochs, num_steps, hidden_dim, \n",
    "                          learning_rate, clipping_theta, batch_size,\n",
    "                          pred_period, pred_len, seqs, get_params, get_inputs,\n",
    "                          ctx, corpus_indices, idx_to_char, char_to_idx,\n",
    "                          is_lstm=False):\n",
    "    if is_random_iter:\n",
    "        data_iter = data_iter_random\n",
    "    else:\n",
    "        data_iter = data_iter_consecutive\n",
    "    params = get_params()\n",
    "    \n",
    "    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        # 如使用相邻批量采样，在同一个epoch中，隐含变量只需要在该epoch开始的时候初始化。\n",
    "        if not is_random_iter:\n",
    "            state_h = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "            if is_lstm:\n",
    "                # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "                state_c = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "        train_loss, num_examples = 0, 0\n",
    "        for data, label in data_iter(corpus_indices, batch_size, num_steps, \n",
    "                                     ctx):\n",
    "            # 如使用随机批量采样，处理每个随机小批量前都需要初始化隐含变量。\n",
    "            if is_random_iter:\n",
    "                state_h = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "                if is_lstm:\n",
    "                    # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "                    state_c = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "            with autograd.record():\n",
    "                # outputs 尺寸：(batch_size, vocab_size)\n",
    "                if is_lstm:\n",
    "                    # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "                    outputs, state_h, state_c = rnn(get_inputs(data), state_h,\n",
    "                                                    state_c, *params) \n",
    "                else:\n",
    "                    outputs, state_h = rnn(get_inputs(data), state_h, *params)\n",
    "                # 设t_ib_j为i时间批量中的j元素:\n",
    "                # label 尺寸：（batch_size * num_steps）\n",
    "                # label = [t_0b_0, t_0b_1, ..., t_1b_0, t_1b_1, ..., ]\n",
    "                label = label.T.reshape((-1,))\n",
    "                # 拼接outputs，尺寸：(batch_size * num_steps, vocab_size)。\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                # 经上述操作，outputs和label已对齐。\n",
    "                loss = softmax_cross_entropy(outputs, label)\n",
    "            loss.backward()\n",
    "\n",
    "            grad_clipping(params, clipping_theta, ctx)\n",
    "            utils.SGD(params, learning_rate)\n",
    "\n",
    "            train_loss += nd.sum(loss).asscalar()\n",
    "            num_examples += loss.size\n",
    "\n",
    "        if e % pred_period == 0:\n",
    "            print(\"Epoch %d. Perplexity %f\" % (e, \n",
    "                                               exp(train_loss/num_examples)))\n",
    "            for seq in seqs:\n",
    "                print ' - ', predict_rnn(rnn, seq, pred_len, params,\n",
    "                      hidden_dim, ctx, idx_to_char, char_to_idx, get_inputs,\n",
    "                      is_lstm)\n",
    "            print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下定义模型参数和预测序列前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "num_steps = 35\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "seq1 = '分开'\n",
    "seq2 = '不分开'\n",
    "seq3 = '战争中部队'\n",
    "seqs = [seq1, seq2, seq3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先采用随机批量采样实验循环神经网络谱写歌词。我们假定谱写歌词的前缀分别为“分开”、“不分开”和“战争中部队”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Perplexity 8.977930\n",
      " -  分开只的证 卞在著天 在是说 别可爱可爱女人 坏坏的说 卞圉宀它 我不想天 一只�\n",
      " -  不分开只 你在后 刱只吃在的说 卞圉宀它 我不想天 一只的证的风 卞我妈妈多 宩我�\n",
      " -  战争中部队 在来不想怪我 别只吃在的说 卞圉宀它 我不想天 一只的证的风 卞我妈妈多 �\n",
      "\n",
      "\n",
      "Epoch 40. Perplexity 4.896211\n",
      " -  分开 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 �\n",
      " -  不分开 皱证我 说散 不能太惏是我想怖的我胳 我的让我疯狂的可爱女人 漂中的脿惏\n",
      " -  战争中部队是我我想怕的我穛 我篮的可爱女人 漂中的脿惏没 它定简我的我穮 我不能太�\n",
      "\n",
      "\n",
      "Epoch 60. Perplexity 2.819769\n",
      " -  分开实你我心颙的我爱你 你分我的你 你你的它却辁 能在这样扛着你的手不懾 你�\n",
      " -  不分开定实 衞你在你想你 那刐 有上不两口 不能再想 我不能再想 我不能再想 我不�\n",
      " -  战争中部队严它漤孩 娞子却依旧每日折一枝杨柳 刱只釠你我忈让 我不如 在叝快巰哇孉 \n",
      "\n",
      "\n",
      "Epoch 80. Perplexity 2.105351\n",
      " -  分开几觉 我该好襽生说 动拉凂没有令宩 你 靠着我可戱女人 坏坏的让我疯狂的可\n",
      " -  不分开到 不也下两不人原劲 是你不想活 河不惏来的太快就却风葴三说那飘门 什� �\n",
      " -  战争中部队但 在方公幞 我的停谥不如隄还那纜 一只令它心仪过�� 一口去忇乡的下头风 \n",
      "\n",
      "\n",
      "Epoch 100. Perplexity 1.774390\n",
      " -  分开姡乡怎么冏 就惏是穦轓 却只冂亡痛 我想渍你有矡免扬 别只想榁多� 无对纆�\n",
      " -  不分开秡景老 战什它 単孥宩漁单！容 打扬多 愑想妁你吃 你乳三两哈 想要和你说�\n",
      " -  战争中部队但巶忠讉釘 僸宦你依不能再想 我不 我不 我不要再想赁 却只秙 沟 在可箩处�\n",
      "\n",
      "\n",
      "Epoch 120. Perplexity 1.610489\n",
      " -  分开劵徹 眨这葌说猻风帻它 失这样牵着你的手不放开 爱能不能够永远单纯粡葉 �\n",
      " -  不分开刚 一你爱手 漀天的原动 有毝早郏你想很久了吧? 我不想 我不想津睥你的时�\n",
      " -  战争中部队 女兺 冏来的让我疯狂的可爱女人 漂亮的让我面红的可爱女人 杏上的话每沩�\n",
      "\n",
      "\n",
      "Epoch 140. Perplexity 1.531393\n",
      " -  分开秀 我想天忙 泡诔禁斑旛处叫僌郳情惉怪穱 一切又劕 刘来爱女人 温柔的让我\n",
      " -  不分开秡 千地箃洔 在睏它之 呫封惌毅女人圲汉堡 别只想躽 说说我戸 不一丠惽夸�\n",
      " -  战争中部队睥不夊 想要咏你的毭 囨陌惹以妈蝜斑奞没 一直厰它忆让情没有刍你 潓你的�\n",
      "\n",
      "\n",
      "Epoch 160. Perplexity 1.461239\n",
      " -  分开劵刑 能怿 僽给爻叅黙 娞子奴人在江南等我 泪不休 语沉默 娘子她人在江南�\n",
      " -  不分开定乭怎么每你疘着步 什盯到腪元手不抛 早遨经 吃愿埋的让刍凈斲靠堅 寏啊 \n",
      " -  战争中部队 眨远让场沉卅 说乊没渍亠 轓说啊 就怎么每日銘绘荐失 釽可爱女人 坏坏的�\n",
      "\n",
      "\n",
      "Epoch 180. Perplexity 1.446572\n",
      " -  分开实事我心我眰箃看 为折三我睛着的箛锾的队友 弃吃缀家徺 葩我碃爰的一只�\n",
      " -  不分开定幞 言觷國迒抨 一颗两颗三颗四颗 连成线背著背黙默许下心伤孥 不纵 温知\n",
      " -  战争中部队 祱这愑劰 馜釂的让刺 霉悹里逎 一只令它心仪的母斑常口汳堦 爱烼你圌那落\n",
      "\n",
      "\n",
      "Epoch 200. Perplexity 1.391650\n",
      " -  分开完很我头的孻力 我们女神襙是呗没有邂了我不如再想 我不 我不 我不能 爱情\n",
      " -  不分开它乽玟紀秀 低秋在远方决斗 在一多被废弃牀我 一直厰朜吕有悉堡 我想带你 \n",
      " -  战争中部队 奅就能想你 道我感动的可爱女人 漂亮的让我面红的可爱女人 漂亮的让我面�\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn=rnn, is_random_iter=True, epochs=200, num_steps=35,\n",
    "                      hidden_dim=hidden_dim, learning_rate=0.2,\n",
    "                      clipping_theta=5, batch_size=32, pred_period=20,\n",
    "                      pred_len=100, seqs=seqs, get_params=get_params,\n",
    "                      get_inputs=get_inputs, ctx=ctx,\n",
    "                      corpus_indices=corpus_indices, idx_to_char=idx_to_char,\n",
    "                      char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再采用相邻批量采样实验循环神经网络谱写歌词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Perplexity 216.204097\n",
      " -  分开 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你\n",
      " -  不分开 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你\n",
      " -  战争中部队 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你的 我不的你\n",
      "\n",
      "Epoch 40. Perplexity 68.837397\n",
      " -  分开 你不要这样你 你知着你不是你 想不是我的爱你 让我们着你的天 我想想你的爱你 不会你的爱 有一种味 不要的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女\n",
      " -  不分开你想要 你不到你有你的你的天 放一种味 我想会这 你想了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你不了这 你\n",
      " -  战争中部队 你不你再不能开 不要的没我想狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏\n",
      "\n",
      "Epoch 60. Perplexity 22.154746\n",
      " -  分开 说你的没太 我的世界 我很着你已 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想\n",
      " -  不分开你走 你不能你爱 我不能再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不\n",
      " -  战争中部队 我说你你已 我不能再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再\n",
      "\n",
      "Epoch 80. Perplexity 9.510521\n",
      " -  分开 这怪是这样的 唱什么我的是你 让我们 半兽人 的灵魂 单纯 对止忿存 永无止尽的战争 让我们 半兽人 的灵魂 单纯 对止忿存 永无止尽的战争 让我们 半兽人 的灵魂 单纯 对止忿存 永无止尽的战争 让\n",
      " -  不分开你走错样) 为是 我想再久了鱼 想是 你想再ㄟ了鱼 龙是 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想\n",
      " -  战争中部队要 你过去着你弃着 你身想慢你离开 难不是我不要你 是因些你 爱你已过去 还没有你 全 一子 你想再ㄟ气质 龙抹 你迷人ㄟ气质 龙抹 你想离久ㄟ鱼 放抹 你想离水ㄟ鱼 龙抹 你想离水ㄟ鱼 龙抹 你想离水\n",
      "\n",
      "Epoch 100. Perplexity 5.283618\n",
      " -  分开 这水的身匙我 一起的最现人 我怎么看不了我不想 你那像全你龙找 一身跳龙 把山我 轻水神 剩一 有杰忿存 永无止尽的战争 让我们 半兽人 的灵魂 单纯 停远古存在永恒 是谁为力忠 你的世界将被摧毁 也\n",
      " -  不分开你 这样爱我放腔水 可只会再手离开 这不是我连分开都迁就着你 我真的没有天份 安静的没这么快 我会学着放弃你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱你 是因为我太爱\n",
      " -  战争中部队 (没有你 你不是过去 怀色蜡壶怎么 景下入秋 漫天黄沙凉过 塞北的客栈人多 牧草有没有 我马儿有些瘦 我就阅老国  什么我不想 你太着 是你不著后我有你太多 这日在然马 我不能 想情了望停才 你在人 \n",
      "\n",
      "Epoch 120. Perplexity 3.581256\n",
      " -  分开 这么的身旧我 不知的叹旧有 过去过去 像使边双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼\n",
      " -  不分开你走错样) 这愿 我想再再一直 龙的话望 在 变人 你想飞ㄟ了质 篮是 我想离水ㄟ鱼 放抹记 剩最后一口气 放抹记 剩抹记 不去 有止忿 x着 li xiu l炭 li xi xi xi xa xi x\n",
      " -  战争中部队一定的天度 我知道一只蜂你没要于我的那画面 经过苏美女神身边 我以不能 还颗心悬在静空 我只能够让分看著 这些看听了还让她知道 我是远会不见 你就着我 说你有 分数怎么停留 一直在停留 谁让它停你的 为\n",
      "\n",
      "Epoch 140. Perplexity 2.896909\n",
      " -  分开 这样的身匙我 一直的我现 这道情依的那一道 想记~~~~~~~~~ 爷 简！简！单！单！ 我想不能去没许 你身你说过我开微没来得  却吹的世的天找 我才你在你的依 一小是人和 我的世界将被摧毁 也许颓\n",
      " -  不分开你走错一定 没在你那经 所以你弃权走撑戏 ㄒー ㄌㄚ ㄙㄡ ㄈㄚ ㄇー 七我讲 风景这呢水 火车叨位去 七我讲 风景这呢水 火车叨位去 七纸它写歌里唱戏 ㄒー ㄌㄚ ㄙㄡ ㄈㄚ ㄇー 七我讲依歌对日我 \n",
      " -  战争中部队家就的你到 你来 想经表懦夫每 龙时 我想离ㄟㄟ鱼 龙抹记 剩抹后一口气 放抹记 剩最后一口气 放抹记 剩抹记 我想就这样牵着你的手不放开 爱可不可以简简单单没有伤害 你 靠着我的肩膀 你 在我胸口睡著\n",
      "\n",
      "Epoch 160. Perplexity 2.448149\n",
      " -  分开 这么的身里我 教着的最旧  是一起ㄟ祷  过i我很是有你说 别可太这样打  就连风吹都要干扰  可是你不想 一直走在黑暗地下道  想吹风想自由想要一起手牵手 去看海绕世界流浪 我害怕你心碎没人帮你擦眼\n",
      " -  不分开怎走 把手不慢在你我 难开球 别给我抬起头 有话去 医到箱锈的画言 说真的风旧出 教静光 看变 这一半 火炉烫 一步 是风人 x炉 li xo人 l炭 xi sou la xi xi xi xi xiu\n",
      " -  战争中部队纵 因要眼这来着就会出面 印象中的爱情好像顶不可见 我给你的爱写在西元前 深埋在美索不达米亚平原 几楔形世字后出土发现 那已风化千年的誓言 一切又重演 听司 神殿 征战 弓箭 是谁的从前 喜欢在人潮中你\n",
      "\n",
      "Epoch 180. Perplexity 2.302149\n",
      " -  分开 这么不在生打 后字都觉 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双\n",
      " -  不分开你走 你说你说爱子我 不开球 快给我抬起头 有话去对医药箱说 别怪我 别怪我 说你怎么面对我 甩开球我满腔的怒火 我想揍你已经很久 别想躲 说你眼睛看着我 别发抖 快给我抬起头 有话去对医药箱说 别怪我\n",
      " -  战争中部队 我爱你你可见 你不着 你爱不 一壶走步 再来一碗热粥 配上几斤的牛肉 我说店小二 三两银够不够 景色入秋 漫天黄沙凉过 塞北的客栈人多 牧草有没有 我马儿有些瘦 天涯尽头 我们上前生奏 我该好好生活 \n",
      "\n",
      "Epoch 200. Perplexity 2.066956\n",
      " -  分开 这么我的爱球 快沸腾 放异么都过去 我在等 灵魂序曲完成 带领族人里下祈祷文 想是轻大路里 让我碰到的只剩模找道 (爱小说 其实我的谁窗好有 什要我多没难分 但根个人已经不 我也耍的有模 快烁检红你 \n",
      " -  不分开怎走 把手慢慢交才 我就着陪国 查什么字眼 有人在降奏 蜥底横著走 三里拽斗牛 别里在篮落 有爽就篮走 别里在角落 有些就要走 三里在斗牛 有些就篮球 有里在篮牛 三种就斗牛 有话在篮牛 有里就篮走 别\n",
      " -  战争中部队然就 我才到你来碎没人帮你擦眼泪 别离那是非只要我们感觉对 我害怕你心碎没人帮你擦眼泪 别离开身非拥有你我的世界才能完美 你们你想逃开 你没有舍不得 你说你也会难过我不相信 牵着你陪着我 也只是曾经 希\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn=rnn, is_random_iter=False, epochs=200, num_steps=35,\n",
    "                      hidden_dim=hidden_dim, learning_rate=0.2,\n",
    "                      clipping_theta=5, batch_size=32, pred_period=20,\n",
    "                      pred_len=100, seqs=seqs, get_params=get_params,\n",
    "                      get_inputs=get_inputs, ctx=ctx,\n",
    "                      corpus_indices=corpus_indices, idx_to_char=idx_to_char,\n",
    "                      char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到一开始学到简单的字符，然后简单的词，接着是复杂点的词，然后看上去似乎像个句子了。\n",
    "\n",
    "## 结论\n",
    "\n",
    "* 通过隐含状态，循环神经网络适合处理前后有依赖关系时序数据样本。\n",
    "* 对前后有依赖关系时序数据样本批量采样时，我们可以使用随机批量采样和相邻批量采样。\n",
    "* 循环神经网络较容易出现梯度衰减和爆炸。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 调调参数（例如数据集大小、序列长度、隐含状态长度和学习率），看看对运行时间、perplexity和预测的结果造成的影响。\n",
    "* 在随机批量采样中，如果在同一个epoch中只把隐含变量在该epoch开始的时候初始化会怎么样？\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/989)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
