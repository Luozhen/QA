{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, nd\n",
    "from mxnet.gluon import nn, rnn, Block\n",
    "from mxnet.contrib import text\n",
    "\n",
    "from io import open\n",
    "import collections\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "BOS = '<bos>'\n",
    "EOS = '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "epoch_period = 10\n",
    "\n",
    "learning_rate = 0.005\n",
    "max_seq_len = 5\n",
    "\n",
    "encoder_num_layers = 1\n",
    "decoder_num_layers = 2\n",
    "\n",
    "encoder_drop_prob = 0.1\n",
    "decoder_drop_prob = 0.1\n",
    "\n",
    "encoder_hidden_dim = 256\n",
    "decoder_hidden_dim = 256\n",
    "aligment_dim = 25\n",
    "\n",
    "ctx = mx.cpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(max_seq_len):\n",
    "    input_tokens = []\n",
    "    output_tokens = []\n",
    "    input_seqs = []\n",
    "    output_seqs = []\n",
    "    \n",
    "    with open('../gluon-tutorials/data/fr-en-small.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            input_seq, output_seq = line.rstrip().split('\\t')\n",
    "            cur_input_tokens = input_seq.split(' ')\n",
    "            cur_output_tokens = output_seq.split(' ')\n",
    "            \n",
    "            if len(cur_input_tokens) < max_seq_len and len(cur_output_tokens) < max_seq_len:\n",
    "                input_tokens.extend(cur_input_tokens)\n",
    "                cur_input_tokens.append(EOS)\n",
    "                while len(cur_input_tokens) < max_seq_len:\n",
    "                    cur_input_tokens.append(PAD)\n",
    "                input_seqs.append(cur_input_tokens)\n",
    "                output_tokens.extend(cur_output_tokens)\n",
    "                cur_output_tokens.append(EOS)\n",
    "                while len(cur_output_tokens) < max_seq_len:\n",
    "                    cur_output_tokens.append(PAD)\n",
    "                output_seqs.append(cur_output_tokens)\n",
    "            \n",
    "        fr_vocab = text.vocab.Vocabulary(collections.Counter(input_tokens),\n",
    "                                         reserved_tokens=[PAD, BOS, EOS])\n",
    "        en_vocab = text.vocab.Vocabulary(collections.Counter(output_tokens),\n",
    "                                         reserved_tokens=[PAD, BOS, EOS])\n",
    "    return fr_vocab, en_vocab, input_seqs, output_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'elle', u'est', u'vieille', u'.', '<eos>'], [u'elle', u'est', u'tranquille', u'.', '<eos>'], [u'elle', u'a', u'tort', u'.', '<eos>'], [u'elle', u'est', u'canadienne', u'.', '<eos>'], [u'elle', u'est', u'japonaise', u'.', '<eos>'], [u'ils', u'sont', u'russes', u'.', '<eos>'], [u'ils', u'se', u'disputent', u'.', '<eos>'], [u'ils', u'regardent', u'.', '<eos>', '<pad>'], [u'ils', u'sont', u'acteurs', u'.', '<eos>'], [u'elles', u'sont', u'crevees', u'.', '<eos>']]\n",
      "\n",
      "[[  5.   6.  21.   4.   3.]\n",
      " [  5.   6.  20.   4.   3.]\n",
      " [  5.   9.  19.   4.   3.]\n",
      " [  5.   6.  11.   4.   3.]\n",
      " [  5.   6.  15.   4.   3.]\n",
      " [  7.   8.  17.   4.   3.]\n",
      " [  7.  18.  13.   4.   3.]\n",
      " [  7.  16.   4.   3.   1.]\n",
      " [  7.   8.  10.   4.   3.]\n",
      " [ 14.   8.  12.   4.   3.]]\n",
      "<NDArray 10x5 @cpu(0)>\n",
      "{u'russes': 17, u'regardent': 16, u'acteurs': 10, '<pad>': 1, u'disputent': 13, u'est': 6, u'crevees': 12, u'.': 4, u'tort': 19, u'japonaise': 15, u'canadienne': 11, u'vieille': 21, u'elle': 5, u'sont': 8, '<eos>': 3, u'a': 9, u'tranquille': 20, '<bos>': 2, u'elles': 14, u'ils': 7, u'se': 18, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "input_vocab, output_vocab, input_seqs, output_seqs = read_data(max_seq_len)\n",
    "X = nd.zeros((len(input_seqs), max_seq_len), ctx=ctx)\n",
    "Y = nd.zeros((len(output_seqs), max_seq_len), ctx=ctx)\n",
    "\n",
    "for i in range(len(input_seqs)):\n",
    "    X[i] = nd.array(input_vocab.to_indices(input_seqs[i]), ctx=ctx)\n",
    "    Y[i] = nd.array(output_vocab.to_indices(output_seqs[i]), ctx=ctx)\n",
    "    \n",
    "dataset = gluon.data.ArrayDataset(X, Y)\n",
    "\n",
    "print input_seqs\n",
    "print X\n",
    "print input_vocab.token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(Block):\n",
    "    \"\"\"编码器\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, drop_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            self.rnn = rnn.GRU(hidden_dim, num_layers, dropout=drop_prob,\n",
    "                              input_size=hidden_dim)\n",
    "    \n",
    "    def forward(self, inputs, state):\n",
    "        # input 尺寸：（1， num_steps）, emb尺寸：(num_steps, 1, 256)\n",
    "        emb = self.embedding(input).swapaxes(0, 1)\n",
    "        emb = self.dropout(emb)\n",
    "        output, state = self.rnn(emb, state)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(Block):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers, max_seq_len,\n",
    "                 drop_prob, alignment_dim, encoder_hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.embdedding = nn.Embedding(output_dim, hidden_dim)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            self.attention = nn.Sequential()\n",
    "            with self.attention.name_scope():\n",
    "                self.attention.add(nn.Dense(alignment_dim, in_units=hidden_dim + encoder_hidden_dim,\n",
    "                    activation=\"tanh\", flatten=False))\n",
    "                self."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
